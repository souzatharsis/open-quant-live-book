[
["index.html", "The Open Quant Live Book Preface", " The Open Quant Live Book OpenQuants.com 2019-12-15 Preface Description The book aims to be an Open Source introductory reference of the most important aspects of financial data analysis, algo trading, portfolio selection, econophysics and machine learning in finance with an emphasis in reproducibility and openness not to be found in most other typical Wall Street-like references. Contribute The Book is Open and we welcome co-authors. Feel free to reach out or simply create a pull request with your contribution! See project structure, guidelines and how to contribute here. Working Contents The Basics I/O Stylized Facts Algo Trading Investment Process Backtesting Factor Investing Limit Order Portfolio Optimization Convex Optimization Risk Parity Portfolios Machine Learning Intro Agent-Based Models Binary Classifiers AutoML Hierarchical Risk Parity Econophysics Entropy, Efficiency and Bubbles Nonparametric Statistical Causality: An Information-Theoretical Approach Financial Networks Alternative Data The Market, The Players, The Rules Case Studies Book’s information First published at: openquants.com. Licensed under Attribution-NonCommercial-ShareAlike 4.0 International. Copyright (c) 2019. OpenQuants.com, New York, NY. "],
["io.html", "Chapter 1 I/O 1.1 Importing Data 1.2 Data Sources 1.3 Conclusion", " Chapter 1 I/O In this Chapter, we will introduce basic functions to read text, excel and JSON files as well as large files. We will also show how to obtain free financial and economic data including the following: End-of-day and real-time pricing; Company financials; Macroeconomic data. Data sources utilized in this Chapter include the following: U.S. Securities and Exchange Commission; Quandl; IEX; Alpha Vantage. 1.1 Importing Data 1.1.1 Text Files The most basic and commonly used option to import data from text files in R is the use of the function read.table from the r-base. We can use this function to read text files with extensions such as .txt and .csv. dat.table &lt;- read.table(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- read.csv(file = &quot;&lt;name of your file&gt;.csv&quot;) The package readr provides functions for reading text data into R that are much faster that the functions from the r-base. The read_table function from the package readr provides a near-replacement for the read.table function. library(readr) dat.table &lt;- readr::read_table2(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- readr::read_csv(file = &quot;&lt;name of your file&gt;.csv&quot;) Another option to save data is to write it in rds format. Data stored in rds format has the advantage to keep the original data struture and type of the object saved. Also, .rds files are compressed and consume less space than files saved in .csv format. A data.frame object can be saved in rds format and then loaded back as follows: write_rds(dat.frame, path = &quot;&lt;name of your file&gt;.rds&quot;) dat.frame &lt;- read_rds(path = &quot;&lt;name of your file&gt;.rds&quot;) 1.1.2 Excel Files The package readxl has an ease to use interface to functions that load excel documents in R. The functions read_xls and read_xlsx can be used to read excel files as follows: library(readxl) readxl::read_xls(path = &quot;&lt;name of your file&gt;.xls&quot;) readxl::read_xlsx(path = &quot;&lt;name of your file&gt;.xlsx&quot;) The function read_excel() automatically detects the extension of the input file as follows: readxl::read_excel(&quot;&lt;name and extension of your file&gt;&quot;, sheet = &quot;&lt;sheet name or index&gt;&quot;) In the read_excel function, the sheet argument can receive either the target sheet name or index number, where sheet indexing starts at 1. The readxl has been oberving increased use compared to other comparable packages such as gdata and the xlsx due to its relative ease of use and performance. Also, the readxl do not have depency with external code libraries while the packages gdata and xlsx depend on ActiveState PERL and the Java JDK, respectively. 1.1.3 JSON Files JSON files are particularly used for transmitting data in web applications but also frequently used as a standard data interchange format. The jsonline package can be used to parse files in JSON format as follows: library(jsonlite) result_json &lt;- read_json(&quot;&lt;json file&gt;&quot;) 1.1.4 Large Files Fast data manipulation in a short and flexible syntax. 1.2 Data Sources In this section, we will show how to obtain financial and economic data from public sources. 1.2.1 Alpha Vantage Alpha Vantage offers free access to pricing data including: Stock Time Series Data; Physical and Digital/Crypto Currencies (e.g., Bitcoin); Technical Indicators and Sector Performances. The data are available in JSON and CSV formats via REST APIs. The quantmod and the alphavantager R packages offer a lightweight R interface to the Alpha Vantage API. Daily stock prices can be obtained with the quantmod::getSymbols function as follows: getSymbols(Symbols=&#39;AAPL&#39;, src=&quot;av&quot;, output.size=&quot;full&quot;, adjusted=TRUE, api.key=&#39;your API key&#39;) The output data is stored in an object with the same name as the corresponding symbol, in this example AAPL. The output data looks like the following AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted 62.8 65.0 62.7 64.8 1.12e+08 2.02 65.0 65.0 62.0 62.1 8.41e+07 1.93 62.6 65.9 62.1 65.2 1.59e+08 2.03 67.0 68.8 66.0 68.6 1.97e+08 2.14 67.6 71.4 66.9 70.6 2.24e+08 2.20 71.9 75.2 70.5 73.5 4.09e+08 2.29 We called the quantmod::getSymbols function with the following arguments: Symbols='AAPL' defines a character vector specifying the names of each symbol to be loaded, here specified by the symbol of the company Apple Inc.; src=&quot;av&quot; specifies the sourcing method, here defined with the value corresponding to Alpha Vantage; output.size=&quot;full&quot;specified length of the time series returned. The strings compact and full are accepted with the following specifications: compact returns only the latest 100 data points; full returns the full-length time series of up to 20 years of historical data; adjusted=TRUE defines a boolean variable to include a column of closing prices adjusted for dividends and splits; api.key specifies your Alpha Vantage API key. 1.2.2 IEX The IEX Group operates the Investors Exchange (IEX), a stock exchange for U.S. equities that is built for investors and companies. IEX offers U.S. reference and market data including end-of-day and intraday pricing data. IEX offers an API with “a set of services designed for developers and engineers. It can be used to build high-quality apps and services”. Data sourced from the IEX API is freely available for commercial subject to conditions and the use of their API is subject to additional terms of use. IEX lists the following github project as an unofficial API for R: https://github.com/imanuelcostigan/iex. We will provide examples on how to obtain intraday pricing data using this package. First, we will use the devtools to install the package directly from its github repository as follows: library(devtools) install_github(&quot;imanuelcostigan/iex&quot;) The iex package provides 4 set of functions as follows: last: Provides IEX near real time last sale price, size and time. Last is ideal for developers that need a lightweight stock quote. IEX API real time API documentation. market: Provides exchange trade volume data in near real time. IEX market API documentation. stats: A set of functions that return trading statistics. IEX stats API documentation. tops: Provides IEX’s aggregated bid and offer position in near real time for all securities on IEX’s displayed limit order book. IEX API TOPS documentation. For instance, the last function has the following arguments: symbols: A vector of tickers (case insensitive). Special characters will be escaped. A list of eligible symbols is published daily by the IEX. When set to NULL (default) returns values for all symbols. fields: A vector of fields names to return (case sensitive). When set to NULL (default) returns values for all fields. version: The API version number, which is used to define the API URL. We can obtain intraday stock price data with the last function as follows: dat &lt;- iex::last(symbols = c(&quot;AAPL&quot;), fields = c(&quot;symbol&quot;, &quot;price&quot;, &quot;size&quot;)) The function returns an S3 object of class iex_api which has three accessible fields: path , response and content. The path contains the corresponding IEX API path: dat$path ## [1] &quot;tops/last&quot; The response contains the unparsed IEX API response: dat$response ## Response [https://api.iextrading.com/1.0/tops/last?symbols=AAPL&amp;filter=symbol%2Cprice%2Csize] ## Date: 2019-08-27 02:04 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 45 B The content contains the parsed content from the API’s response: dat$content ## [[1]] ## [[1]]$symbol ## [1] &quot;AAPL&quot; ## ## [[1]]$price ## [1] 207 ## ## [[1]]$size ## [1] 100 According to the developer, this package causes R to pause 0.2 seconds after executing an API call to avoid the user being throttled by the IEX API (which enforces a 5 request per second limit). Documentation about the other set of functions can be obtained at https://github.com/imanuelcostigan/iex/tree/master/man. 1.2.3 Quandl Quandl is likely the largest financial and alternative data aggregator/provider today. They leverage relationships with third-party providers to be a one-stop-shop for alternative data and traditional fundamental, pricing and estimates datasets. Quandl offer an API which usage is free for registered users. You can obtain an API key here. After signing up, just append your API key to your call like this: https://www.quandl.com/api/v3/datasets/WIKI/FB/data.csv?api_key=YOURAPIKEYHERE At Quandl, every dataset is identified by “Quandl code”, which is a unique id. In the above example, you downloaded a dataset with the Quandl code “WIKI/FB”. Every Quandl code has 2 parts: the database code (“WIKI”) which specifies where the data comes from, and the dataset code (“FB”) which identifies the specific time series you want. You can find Quandl codes using their data browser. Additional API documentation can be found here. Quandl is also available via an R interface (Raymond McTaggart, Gergely Daroczi, and Clement Leung 2019). For instance, we can obtain Crude Oil Futures prices from 01/01/2010 to 01/01/2019 as follows: library(Quandl) Quandl.api_key(config::get()$quandl.key) from.dat &lt;- as.Date(&quot;01/01/2010&quot;, format=&quot;%d/%m/%Y&quot;) to.dat &lt;- as.Date(&quot;01/01/2019&quot;, format=&quot;%d/%m/%Y&quot;) crude.oil.futures&lt;-Quandl(&quot;CHRIS/CME_CL1&quot;, start_date = from.dat, end_date = to.dat, type=&quot;xts&quot;) plot(crude.oil.futures$Last) In the example above we specified the following Database/Dataset: Database: “CHRIS”. Continuous contracts for all 600 futures on Quandl. Built on top of raw data from CME, ICE, LIFFE etc. Curated by the Quandl community. 50 years history. Dataset: “CME_CL1”. Historical futures prices of Crude Oil Futures, Continuous Contract #1. Non-adjusted price based on spot-month continuous contract calculations. Raw data from CME. 1.2.4 SEC Official filings are freely available from the U.S. Securities and Exchange Commission’s EDGAR database. The package finreportr provides an interface in R to facilitate financial analysis from SEC’s 10K and 10K/A filings. We can obtain company basic information with the function the CompanyInfo function by passing the ticker symbol of the target company as follows: library(&quot;finreportr&quot;) AAPL.Info&lt;-CompanyInfo(&quot;AAPL&quot;) print(AAPL.Info) ## company CIK SIC state state.inc FY.end street.address ## 1 Apple Inc. 0000320193 3571 CA CA 0930 ONE APPLE PARK WAY ## city.state ## 1 CUPERTINO CA 95014 As a result, we obtain the following information: Company name: Apple Inc.; SEC Central Index Key (CIK): 0000320193; Standard Industrial Classification (SIC): 3571, which is the industry code for Electronic Computers; Address: ONE APPLE PARK WAY, CUPERTINO CA 95014; Most recent period of report end is 0930. The list of company annual reports with corresponding filing dates can be obtained with the function AnnualReports as follows: AAPL.reports&lt;-AnnualReports(&quot;AAPL&quot;) Table 1.1: Sample Annual Reports filing.name filing.date accession.no 10-K 2018-11-05 0000320193-18-000145 10-K 2017-11-03 0000320193-17-000070 10-K 2016-10-26 0001628280-16-020309 10-K 2015-10-28 0001193125-15-356351 10-K 2014-10-27 0001193125-14-383437 10-K 2013-10-30 0001193125-13-416534 The accession number is a unique identifier that the SEC creates for each filing. Company financials are organized into 3 segments: Income Statement, Balance Sheet and Cash Flow. Income Statement Financials from the Income Statement segment can be obtained with the GetIncome function as follows: AAPL.IS&lt;-GetIncome(&quot;AAPL&quot;, 2017) Table 1.2: Sample Income Statement Financials Metric Units Amount startDate endDate Revenue, Net usd 233715000000 2014-09-28 2015-09-26 Revenue, Net usd 75872000000 2015-09-27 2015-12-26 Revenue, Net usd 50557000000 2015-12-27 2016-03-26 Revenue, Net usd 42358000000 2016-03-27 2016-06-25 Revenue, Net usd 46852000000 2016-06-26 2016-09-24 Revenue, Net usd 215639000000 2015-09-27 2016-09-24 The Income Statement function returns data for the following metrics: Table 1.3: Income Statement Metrics Metrics Revenue, Net Cost of Goods and Services Sold Gross Profit Research and Development Expense Selling, General and Administrative Expense Operating Expenses Operating Income (Loss) Nonoperating Income (Expense) Income (Loss) from Continuing Operations before Income Taxes, Noncontrolling Interest Income Tax Expense (Benefit) Net Income (Loss) Attributable to Parent Earnings Per Share, Basic Earnings Per Share, Diluted Weighted Average Number of Shares Outstanding, Basic Weighted Average Number of Shares Outstanding, Diluted Common Stock, Dividends, Per Share, Declared Balance Sheet Financials from the Balance Sheet segment can be obtained with the GetBalanceSheet function as follows: AAPL.BS&lt;-GetBalanceSheet(&quot;AAPL&quot;, 2017) Table 1.4: Sample Balance Sheet Financials Metric Units Amount startDate endDate Cash and Cash Equivalents, at Carrying Value usd 13844000000 NA 2014-09-27 Cash and Cash Equivalents, at Carrying Value usd 21120000000 NA 2015-09-26 Cash and Cash Equivalents, at Carrying Value usd 20484000000 NA 2016-09-24 Cash and Cash Equivalents, at Carrying Value usd 20289000000 NA 2017-09-30 Available-for-sale Securities, Current usd 46671000000 NA 2016-09-24 Available-for-sale Securities, Current usd 53892000000 NA 2017-09-30 The Balance Sheet function returns data for the following metrics: Table 1.5: Balance Sheet Metrics Metrics Cash and Cash Equivalents, at Carrying Value Available-for-sale Securities, Current Accounts Receivable, Net, Current Inventory, Net Nontrade Receivables, Current Other Assets, Current Assets, Current Available-for-sale Securities, Noncurrent Property, Plant and Equipment, Net Goodwill Intangible Assets, Net (Excluding Goodwill) Other Assets, Noncurrent Assets Accounts Payable, Current Accrued Liabilities, Current Deferred Revenue, Current Commercial Paper Long-term Debt, Current Maturities Liabilities, Current Deferred Revenue, Noncurrent Long-term Debt, Excluding Current Maturities Other Liabilities, Noncurrent Liabilities Commitments and Contingencies Common Stocks, Including Additional Paid in Capital Retained Earnings (Accumulated Deficit) Accumulated Other Comprehensive Income (Loss), Net of Tax Stockholders’ Equity Attributable to Parent Liabilities and Equity Cash Flow Financials from the Cash Flow segment can be obtained with the GetCashFlow function as follows: AAPL.CF&lt;-GetCashFlow(&quot;AAPL&quot;, 2017) Table 1.6: Sample Cash Flow Financials Metric Units Amount startDate endDate Cash and Cash Equivalents, at Carrying Value usd 13844000000 NA 2014-09-27 Cash and Cash Equivalents, at Carrying Value usd 21120000000 NA 2015-09-26 Cash and Cash Equivalents, at Carrying Value usd 20484000000 NA 2016-09-24 Cash and Cash Equivalents, at Carrying Value usd 20289000000 NA 2017-09-30 Net Income (Loss) Attributable to Parent usd 53394000000 2014-09-28 2015-09-26 Net Income (Loss) Attributable to Parent usd 18361000000 2015-09-27 2015-12-26 The Cash Flow function returns data for the following metrics: Table 1.7: Cash Flow Metrics Metrics Cash and Cash Equivalents, at Carrying Value Net Income (Loss) Attributable to Parent Depreciation, Amortization and Accretion, Net Share-based Compensation Deferred Income Tax Expense (Benefit) Other Noncash Income (Expense) Increase (Decrease) in Accounts Receivable Increase (Decrease) in Inventories Increase (Decrease) in Other Receivables Increase (Decrease) in Other Operating Assets Increase (Decrease) in Accounts Payable Increase (Decrease) in Deferred Revenue Increase (Decrease) in Other Operating Liabilities Net Cash Provided by (Used in) Operating Activities Payments to Acquire Available-for-sale Securities Proceeds from Maturities, Prepayments and Calls of Available-for-sale Securities Proceeds from Sale of Available-for-sale Securities Payments to Acquire Businesses, Net of Cash Acquired Payments to Acquire Property, Plant, and Equipment Payments to Acquire Intangible Assets Payments to Acquire Other Investments Payments for (Proceeds from) Other Investing Activities Net Cash Provided by (Used in) Investing Activities Proceeds from Issuance of Common Stock Excess Tax Benefit from Share-based Compensation, Financing Activities Payments Related to Tax Withholding for Share-based Compensation Payments of Dividends Payments for Repurchase of Common Stock Proceeds from Issuance of Long-term Debt Repayments of Long-term Debt Proceeds from (Repayments of) Commercial Paper Net Cash Provided by (Used in) Financing Activities Cash and Cash Equivalents, Period Increase (Decrease) Income Taxes Paid, Net Interest Paid 1.3 Conclusion We showed how to load and import data from both local files and external sources. We provided examples on how to read tabular data and how to handle large files. We showed how to obtain financial and economic data from freely available sources. 1.3.1 Further Reading To further learn how to use R to load, transform, visualize and model data see (Wickham and Grolemund 2017). Additional relevant R packages include the following: dplyr: Fast data frames manipulation and database query. reshape2: Flexibly rearrange, reshape and aggregate data. readr: A fast and friendly way to read tabular data into R. tidyr: Easily tidy data with spread and gather functions. rlist: A toolbox for non-tabular data manipulation with lists. jsonlite: A robust and quick way to parse JSON files in R. ff: Data structures designed to store large datasets. lubridate: A set of functions to work with dates and times. References "],
["stylized-facts.html", "Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.3 Volatility 2.4 Correlation", " Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.2.1 Fat Tails 2.2.2 Skewness 2.3 Volatility 2.3.1 Time-invariance 2.3.2 Volatility Clustering 2.3.3 Correlation with Trading Volume 2.4 Correlation 2.4.1 Time-invariance 2.4.2 Auto-correlation "],
["risk-parity-portfolios.html", "Chapter 3 Risk Parity Portfolios 3.1 Introduction 3.2 Risk Parity Portfolio 3.3 Tangency Portfolio 3.4 Optimizing FAANG: Ray Dalio versus Markowitz 3.5 Discussion and Conclusion", " Chapter 3 Risk Parity Portfolios 3.1 Introduction The “risk parity” approach was popularized by Ray Dalio’s Bridgewater Associates - the largest hedge fund by assets under management ($132.8 billions of USD) - with the creation of the All Weather asset allocation strategy in 1996. “All Weather” is a term used to designate funds that tend to perform reasonably well during both favorable and unfavorable economic and market conditions. Today, several managers have employed “All Weather” concepts under a risk parity approach. Figure 3.1: 7 November 2018; Ray Dalio, Bridgewater Associates on Centre Stage during day two of Web Summit 2018 at the Altice Arena in Lisbon, Portugal. Photo by David Fitzgerald/Web Summit via SportsfilePhoto by David Fitzgerald /Sportsfile. A risk parity portfolio seeks to achieve an equal balance between the risk associated with each asset class or portfolio component. In that way, lower risk asset classes will generally have higher notional allocations than higher risk asset classes. Risk Parity is about Balance - Bridgewater. Risk parity strategies suffered in recent history (2010-2017) as the bull market has pushed stocks to a record high hence favoring equity-concentrated portfolios. However, the increase in market volatility since 2018, the emergency of geo-political and tradewars risk as well as the growth in haven assets like Gold create conditions that strengthen the case for diversified portfolios. This is demonstrated in Fig. 3.2 which shows that the S&amp;P risk parity strategy has returned almost 10% over the last 12 months (Aug/2018 - Aug-2019), more than double the S&amp;P 500 index of U.S. stocks. Figure 3.2: S&amp;P 500 index versus S&amp;P Risk Parity Index. Source: Bloomberg. In Aug/2019, there have been news about the launch of a new Risk Parity ETF in the US. The RPAR Risk Parity ETF plans to allocate across asset classes based on risk, regulatory filings show. The fund would be the first in the U.S. to follow this quantitative approach, allotting more money to securities with lower volatility according to Bloomberg. [The RPAR Risk Parity ETF is] kind of like Bridgewater does, but they just do it for the wealthiest institutions in the world. The idea here is to build something that would work for everybody. - Alex Shahidi, former relationship manager at Dalio’s Bridgewater Associate and creator of the RPAR Risk Parity ETF. Bloomberg. But how can we a risk parity portfolio? How does it perform against a traditional mean/variance model? In this Chapter, We will show how you can build your own Risk Parity portfolio We will create and compare the performance two indices: A FAANG Risk Parity Index of FAANG companies with equal risk balance A FAANG Tangency Portfolio Index of FAANG companies with weights such that return/risk ratio is maximized By the end of the Chapter, you will be able to create your own risk parity / All Weather fund and compare it against your benchmark of choice. 3.2 Risk Parity Portfolio A risk parity portfolio denotes a class of portfolios whose assets verify the following equalities (Vinicius and Palomar 2019): \\[\\begin{equation} w_{i} \\frac{\\partial f(\\mathbf{w})}{\\partial w_{i}}=w_{j} \\frac{\\partial f(\\mathbf{w})}{\\partial w_{j}}, \\forall i, j \\end{equation}\\] where \\(f\\) is a positively homogeneous function of degree one that measures the total risk of the portfolio and \\(\\mathbf{w}\\) is the portfolio weight vector. In other words, the marginal risk contributions for every asset in a risk parity portfolio are equal. A common choice for \\(f\\), for instance, is the standard deviation of the portfolio, which is usually called volatility, i.e., \\(f(\\mathbf{w})=\\sqrt{\\mathbf{w}^{T} \\mathbf{\\Sigma} \\mathbf{w}}\\), where \\(\\mathbf{\\Sigma}\\) is the covariance matrix of assets. In practice, risk and portfolio managers have risk mandates they follow or bounds for marginal risk contributions at the asset, country, regional or sector levels. Hence, a natural extension of the risk parity portfolio is the so called risk budget portfolio, in which the marginal risk contributions match preassigned quantities (Vinicius and Palomar 2019). Mathematically, \\[\\begin{equation} w_{i}(\\Sigma \\mathbf{w})_{i}=b_{i} \\mathbf{w}^{T} \\Sigma \\mathbf{w}, \\forall i, \\end{equation}\\] where \\(\\mathbf{b} \\triangleq\\left(b_{1}, b_{2}, \\ldots, b_{N}\\right)\\left(\\text { with } \\mathbf{1}^{T} \\mathbf{b}=1 \\text { and } \\mathbf{b} \\geq \\mathbf{0}\\right)\\) is the vector of desired marginal risk contributions. 3.3 Tangency Portfolio Mean variance optimization is a commonly used quantitative tool part of Modern Portfolio Theory that allows investors to perform allocation by considering the trade-off between risk and return. Figure 3.3: In 1990, Dr. Harry M. Markowitz shared The Nobel Prize in Economics for his work on portfolio theory. In a mean-variance framework, the objective is to minimize portfolio risk \\(\\sigma^2\\) subject to a baseline expected rate of return \\(\\mu_b\\) as follows: \\[\\begin{equation} \\begin{array}{ll}{\\mathcal{M}} &amp; {\\text { minimize } \\quad \\frac{1}{2} w^{T} \\Sigma w} \\\\ {\\text { subject to }} &amp; {\\mathrm{m}^{T} w \\geq \\mu_{b}, \\text { and } \\mathbf{1}^{T} w=1}\\end{array} \\end{equation}\\] where \\(m\\) is the vector of expected returns for the portfolio assets. We will obtain an optimal portfolio (min risk) for each target rate of return \\(\\mu_b\\) thus forming an efficient frontier. Each point in the efficient frontier in Fig. 3.4 is a portfolio with an optimal combination of securities that minimized risk given a level of risk (standard deviation). The dots below the efficient frontier are portfolios with inferior performance. They either offer the same returns but with higher risk, or they offer less return for the same risk. Figure 3.4: Efficienty Frontier. Attribution: ShuBraque (CC BY-SA 3.0) But how can we choose a portfolio from the efficient frontier? One approach is to choose the most efficient portfolio from a risk/return standpoint, i.e., the portfolio with the highest Sharpe ratio (ratio between excess return and portfolio standard deviation). This portfolio is called the tangency portfolio and it’s located at the tangency point of the Capital Allocation Line and the Efficient Frontier. We will implement both a parity risk and a tangency portfolio in the next section. 3.4 Optimizing FAANG: Ray Dalio versus Markowitz 3.4.1 Single Portfolio First, we will load log-returns of adjusted prices for FAANG companies, i.e., the stocks identified by the following tickers: FB, AMZN, AAPL, NFLX and GOOG (see Appendix B.2 for code used to generate this dataset). library(xts) # load FAANG returns faang.returns&lt;-as.xts(read.zoo(&#39;./data/FAANG.csv&#39;, header=TRUE, index.column=1, sep=&quot;,&quot;)) We can use the packages riskParityPortfolio and fPortfolio to build a FAANG risk parity and tangency portfolios, respectively. We will first consider FAANG returns from 2018 to build the portfolios as follows: library(IDPmisc) library(riskParityPortfolio) library(fPortfolio) # consider returns from 2018 # omit days with missing data (INF/NA returns) faang.returns.filtered &lt;- NaRV.omit(as.matrix(faang.returns[&quot;2018&quot;])) # calculate covariance matrix Sigma &lt;- cov(faang.returns.filtered) # compute risk parity portfolio portfolio.parity &lt;- riskParityPortfolio(Sigma) # compute tangency portfolio portfolio.tangency &lt;- tangencyPortfolio(as.timeSeries(faang.returns.filtered), constraints = &quot;LongOnly&quot;) portfolio.weights &lt;- rbind(portfolio.parity$w, getWeights(portfolio.tangency)) row.names(portfolio.weights)&lt;-c(&quot;Parity Portfolio&quot;, &quot;Tangency Portfolio&quot;) Fig. 3.5 shows the portfolio weights obtained for both the Parity and the Tangency portfolios. We observe that the Tangency portfolio concentrates the weights between Amazon and Netflix with both companies having nearly the same weight while Facebook, Apple and Google are left out of the portfolio. On the other hand, the Parity portfolio presents a well-balanced distribution of weights among the FAANG companies with all company weights around 20%. Apple and Google have weights a little over 20% while Netflix is the company with the lowest weight (15%). barplot(portfolio.weights, main = &quot;&quot;, xlab = &quot;stocks&quot;, ylab = &quot;dollars&quot;, beside = TRUE, legend = TRUE, col=c(&quot;black&quot;, &quot;red&quot;), args.legend = list(bg = &quot;white&quot;)) Figure 3.5: Portfolio weights for parity and tangency FAANG portfolios considering returns from 2018. Fig. 3.6 compares the (covariance) risk budget of the Parity and Tangency portfolios obtained. As expected, we observe that the Parity portfolio has a risk budget equally distributed among the portfolio assets. On the other hand, the Tangency portfolio concentrates the risk between Amazon and Netflix with the latter corresponding to over 56% of the risk budget of the portfolio. portfolio.risks &lt;- rbind(portfolio.parity$risk_contribution/sum(portfolio.parity$risk_contribution), getCovRiskBudgets(portfolio.tangency)) row.names(portfolio.risks)&lt;-c(&quot;Parity Portfolio&quot;, &quot;Tangency Portfolio&quot;) barplot(portfolio.risks, main = &quot;&quot;, xlab = &quot;stocks&quot;, ylab = &quot;Covariance Risk Budget&quot;, beside = TRUE, legend = TRUE, col=c(&quot;black&quot;, &quot;red&quot;), args.legend = list(bg = &quot;white&quot;)) Figure 3.6: Portfolio covariance risk budget for parity and tangency FAANG portfolios considering returns from 2018. 3.4.2 The Ray Dalio FAANG Index What would be the performance of a “Ray Dalio FAANG Index” i.e. a portfolio composed of FAANG companies and rebalanced to match a corresponding Risk Parity portfolio? Would it beat a corresponding Tagency portfolio? To answer these questions, we will consider a portfolio of FAANG companies in the time period from 2014-01-01 and 2019-09-01 and build two indices: Risk Parity Index: Rebalances portfolio weights quarterly setting the weights according to a risk parity portfolio; Tangency Portfolio Index: Rebalances portfolio weights quarterly setting weights according to a Tangency portfolio. We first define our rebalance dates by constructing a rolling window of 12-month width and a 3-month step-size as follows: library(fPortfolio) faang.returns.xts&lt;-faang.returns[&quot;2014-01-01/2019-09-01&quot;] rWindows&lt;-rollingWindows(faang.returns.xts, period=&quot;12m&quot;, by=&quot;3m&quot;) Our rebalance dates are the following: print(rWindows$to) ## GMT ## [1] [2014-12-31] [2015-03-31] [2015-06-30] [2015-09-30] [2015-12-31] ## [6] [2016-03-31] [2016-06-30] [2016-09-30] [2016-12-31] [2017-03-31] ## [11] [2017-06-30] [2017-09-30] [2017-12-31] [2018-03-31] [2018-06-30] ## [16] [2018-09-30] [2018-12-31] [2019-03-31] [2019-06-30] Next, we calculate risk parity portfolio weights at each rebalance date considering returns in a 12-month window as follows: # Apply FUN to time-series R in the subset [from, to]. ApplyFilter &lt;- function(from, to, R, FUN){ return(FUN(R[paste0(from, &quot;/&quot;, to)])) } # For each pair (from, to) ApplyFilter to time-series R using FUN ApplyRolling &lt;- function(from, to, R, FUN){ library(purrr) return(map2(from, to, ApplyFilter, R=R, FUN=FUN)) } # Returns weights of a risk parity portfolio from covariance matrix of matrix of returns r CalculateRiskParity &lt;- function(r){ library(riskParityPortfolio) return(riskParityPortfolio(cov(r))$w) } # Given a matrix of returns `r`, # calculates risk parity weights for each date in `to` considering a time window from `from` and `to` RollingRiskParity &lt;- function(from, to, r){ library(rlist) p&lt;-ApplyRolling(from, to, r, CalculateRiskParity) names(p)&lt;-to return(list.rbind(p)) } parity.weights&lt;-RollingRiskParity(rWindows$from@Data, rWindows$to@Data, faang.returns.xts) We now calculate quarterly weights for FAANG tangency portfolios. We leverage the fPortfolio package to calculate a rolling tangency portfolio as follows: library(fPortfolio) faang.returns.ts&lt;-as.timeSeries(faang.returns.xts) Spec = portfolioSpec() rolling.portfolio.tangency &lt;- rollingTangencyPortfolio(faang.returns.ts, constraints = &quot;LongOnly&quot;, from=rWindows$from, to=rWindows$to, spec=Spec) names(rolling.portfolio.tangency)&lt;-rWindows$to tan.weights &lt;- sapply(rolling.portfolio.tangency,getWeights) rownames(tan.weights) &lt;- colnames(faang.returns.ts) tan.weights&lt;-t(tan.weights) Figs. 3.7 and 3.8 show the portfolio weights obtained for parity risk and tangency portfolios, respectively. We observe that the risk parity weights are quite stable over time with Netflix having a slightly underweighting compared to the other portfolio constituents. On the other hand, the tangency portfolio weights vary considerably throughout the time period considered, which can impose challenges in its maintenance as its turnover can be quite high. The tangency portfolio overweights Apple and Amazon across many rebalance dates and it underweights Google in all rebalance dates. PerformanceAnalytics::chart.StackedBar(parity.weights, xlab = &quot;Rebalance Dates&quot;, ylab = &quot;Weight&quot;, main = &quot;FAANG Risk Parity&quot;) Figure 3.7: Portfolio weights for FAANG risk parity portfolios. PerformanceAnalytics::chart.StackedBar(tan.weights, xlab = &quot;Rebalance Dates&quot;, ylab = &quot;Weight&quot;, main = &quot;FAANG Tangency Portfolio&quot;) Figure 3.8: Portfolio weights for FAANG tangency portfolios. We will use the time series of FAANG companies and the time series of risk parity and tangency portfolio weights to calculate the returns of the risk parity and tangency portfolio indexes as follows: library(PerformanceAnalytics) tan.returns &lt;- Return.portfolio(faang.returns.xts, weights=tan.weights,verbose=TRUE) parity.returns &lt;- Return.portfolio(faang.returns.xts, weights=parity.weights,verbose=TRUE) p.returns&lt;-merge(tan.returns$returns, parity.returns$returns) names(p.returns)&lt;-c(&quot;FAANG Tangency Index&quot;, &quot;FAANG Parity Index&quot;) Fig. 3.9 shows the performance summary for the risk parity index versus the tangency portfolio index. Surprisingly, the FAANG risk parity index outperforms the FAANG tangency portfolio index by quite a bit with a cumulative return of 169.482% versus 109.652% from the tangency portfolio index. The FAANG risk parity index also has a relatively lower drawdown across most of the period analyzed. ### Performance Summary (return / drawdown) PerformanceAnalytics::charts.PerformanceSummary(p.returns, colorset=rich6equal, lwd=2, cex.legend = 1.0, event.labels = TRUE, main = &quot;&quot;) Figure 3.9: Performance summary for the risk parity index versus the tangency portfolio index Tables 3.1 and 3.2 show the calendar returns for the risk parity and tangency portfolio indexes, respectively. Interestingly, in years where the tangency portfolio index had positive cumulative return, the risk parity index yielded less returns than the tangency portfolio index. Conversely, in years where the tangency portfolio index had negative cumulative return, the risk parity index showed superior performance than the tangency portfolio index. In that way, the risk parity index showed “not as good” but also “not as bad” yearly returns compared to the tangency portfolios. Table 3.1: Calendar Returns (%): FAANG Parity Index 2015 2016 2017 2018 2019 Jan 2.1 0.6 2.1 -0.6 -1.2 Feb -1.1 3.7 1.4 -1.6 1.0 Mar -0.6 1.3 0.0 2.3 1.6 Apr 0.9 1.7 1.7 1.4 0.9 May 0.4 -0.6 0.1 1.8 -2.2 Jun 0.6 1.3 -0.4 -0.4 1.3 Jul -0.4 1.3 0.5 1.8 -1.1 Aug -4.2 0.2 -0.1 -0.2 -0.4 Sep 0.9 0.7 0.8 0.3 NA Oct -0.7 -1.0 0.2 1.7 NA Nov 1.8 -1.2 -0.9 0.3 NA Dec -1.8 -1.3 -0.8 0.8 NA FAANG Parity Index -2.1 6.9 4.6 7.8 0.0 Table 3.2: Calendar Returns (%): FAANG Tangency Index 2015 2016 2017 2018 2019 Jan -1.7 -1.0 4.5 0.6 -2.6 Feb -1.6 4.8 1.7 -1.4 0.8 Mar -0.3 1.5 0.0 2.4 2.4 Apr 2.8 5.0 2.3 0.7 0.5 May 0.3 -0.5 0.2 1.4 -2.1 Jun 1.0 2.1 -0.4 -0.5 1.6 Jul -0.4 1.1 0.7 0.6 -1.2 Aug -4.6 0.2 0.0 -0.3 -0.4 Sep 0.4 0.9 0.6 1.1 NA Oct 0.5 -0.7 -0.4 3.6 NA Nov 2.0 -1.3 -0.5 0.5 NA Dec -1.9 -1.8 -0.9 1.7 NA FAANG Tangency Index -3.7 10.3 7.9 11.0 -1.2 Fig. 3.10 shows the performance summary in a rolling 252-day window. Again, we observe that the risk parity index presents a superior performance compared to the tangency portfolio index. The risk parity index presents higher annualized return, lower standard deviation and superior Sharpe ratio in most of the period analyzed compared to the tangency portfolio index. As presented in Tab. 3.3, the risk parity index has a total of 23.71% annualized return, 22.55% standard deviation and 1.051 Sharpe-ratio versus 17.22% annualized return, 26.42% standard deviation and 0.652 Sharpe-ratio from the tangency portfolio index. Figure 3.10: Performance summary in a rolling 252-day window for the risk parity index versus the tangency portfolio index Table 3.3: Annualized Returns FAANG Tangency Index FAANG Parity Index Annualized Return 0.172 0.237 Annualized Std Dev 0.264 0.226 Annualized Sharpe (Rf=0%) 0.652 1.051 3.5 Discussion and Conclusion What mix of assets has the best chance of delivering good returns over time through all economic environments? That was the question posed by Bridgewater Associates before creating the All Weather funds with concepts today popularized in the so-called risk parity strategies. The traditional approach to asset allocation often tolerates higher concentration of risk with the objective to generate higher longer-term returns. Bridgewater argues that this approach has a serious flaw: If the source of short-term risk is a heavy concentration in a single type of asset, this approach brings with it a significant risk of poor long-term returns that threatens the ability to meet future obligations. This is because every asset is susceptible to poor performance that can last for a decade or more, caused by a sustained shift in the economic environment - Bridgewater. In this Chapter, we introduced the concept of risk parity portfolios and compare it against a mean-variance model. We provided a simple practical example by constructing a FAANG risk parity index and comparing its performance against a FAANG tangency index, which selects the portfolio from the mean-variance efficient frontier with optimal Sharpe-ratio. The risk parity index presented higher annualized return, lower standard deviation and superior Sharpe ratio in most of the period analyzed compared to the tangency portfolio index. Of course, results should be taken with caution. In practice, both the risk parity and mean-variance approaches are employed in larger portfolios potentially across multiple asset classes. Those methodologies strive when there are assets that are uncorrelated in the portfolio which can increase the potential for diversification. Further, modern portfolio optimization strategies can be much more complex with a variety of objective functions and constraints. Our objective in this article was to give you a head start. Feel free to check out the source code in our github project and implement your own strategies! References "],
["entropy.html", "Chapter 4 Entropy 4.1 Definition 4.2 Nonlinear Coupling 4.3 Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets", " Chapter 4 Entropy 4.1 Definition Let \\(X\\) be a random variable and \\(P_X(x)\\) be its probability density function (pdf). The entropy \\(H(X)\\) can be interpreted sa measure of the uncertainty of \\(X\\) and is defined in the discrete case as follows: \\[\\begin{equation} H(X) = -\\sum_{x \\in X}{P_X(x)\\log{P_X(x)}}. \\label{eq:H} \\end{equation}\\] If the \\(\\log\\) is taken to base two, then the unit of \\(H\\) is the (binary digit). We employ the natural logarithm which implies the unit in (natural unit of information). 4.2 Nonlinear Coupling 4.2.1 Simulated Systems 4.2.2 Equity-Commodities Relationship 4.3 Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets "],
["how-to-measure-statistical-causality-a-transfer-entropy-approach-with-financial-applications.html", "Chapter 5 How to Measure Statistical Causality: A Transfer Entropy Approach with Financial Applications 5.1 A First Definition of Causality 5.2 A Probabilistic-Based Definition 5.3 Transfer Entropy and Statistical Causality 5.4 Net Information Flow 5.5 The Link Between Granger-causality and Transfer Entropy 5.6 Information Flow on Simulated Systems 5.7 Information Flow among International Stock Market Indices 5.8 Other Applications 5.9 Conclusions", " Chapter 5 How to Measure Statistical Causality: A Transfer Entropy Approach with Financial Applications We’ve all heard the say “correlation does not imply causation”, but how can we quantify causation? This is an extremely difficult and often misleading task, particularly when trying to infer causality from observational data and we cannot perform controlled trials or A/B testing. Take for example the 2-dimensional system from Fig. 5.1. Figure 5.1: Life is Random (or Nonlinear?) At a first glance, one could say that there is no clear relationship or causality between the random variables \\(x_1\\) and \\(x_2\\). However, this apparent random system presents a causal relationship defined by the following simple equations: \\[\\begin{align*} x_1(n) &amp;= 0.441x_1(n-1) + \\epsilon_1 \\\\ x_2(n) &amp;= 0.51x_1^2(n-1) + \\epsilon_2, \\\\ &amp;\\epsilon_1, \\epsilon_2 \\sim \\mathcal{N}(0,1). \\end{align*}\\] A simple nonlinearity introduced in the relationship between \\(x_2\\) and \\(x_1\\) was enough to introduce complexity into the system and potentially mislead a naive (non-quant) human. Fortunately, we can take advantage of statistics and information theory to uncover complex causal relationships from observational data (remember, this is still a very challenging task). The objectives of this Chapter are the following: Introduce a prediction-based definition of causality and its implementation using a vector auto-regression formulation. Introduce a probabilistic-definition of causality and its implementation using an information-theoretical framework. Simulate linear and nonlinear systems and uncover causal links with the proposed methods. Quantify information flow among global equity indexes further uncovering which indexes are driving the global financial markets. Discuss further applications including the impact of social media sentiment in financial and crypto markets. 5.1 A First Definition of Causality We quantify causality by using the notion of the causal relation introduced by Granger (Wiener 1956; Granger 1969), where a signal \\(X\\) is said to Granger-cause \\(Y\\) if the future realizations of \\(Y\\) can be better explained using the past information from \\(X\\) and \\(Y\\) rather than \\(Y\\) alone. Figure 5.2: Economist Clive Granger, who won the 2003 Nobel Prize in Economics. The most common definitions of Granger-causality (G-causality) rely on the prediction of a future value of the variable \\(Y\\) by using the past values of \\(X\\) and \\(Y\\) itself. In that form, \\(X\\) is said to G-cause \\(Y\\) if the use of \\(X\\) improves the prediction of \\(Y\\). Let \\(X_t\\) be a random variable associated at time \\(t\\) while \\(X^t\\) represents the collection of random variables up to time \\(t\\). We consider \\({X_t}, {Y_t}\\) and \\({Z_t}\\) to be three stochastic processes. Let \\(\\hat Y_{t+1}\\) be a predictor for the value of the variable \\(Y\\) at time \\(t+1\\). We compare the expected value of a loss function \\(g(e)\\) with the error \\(e=\\hat{Y}_{t+1} - Y_{t+1}\\) of two models: The expected value of the prediction error given only \\(Y^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_1(X^{t},Z^t))] \\end{equation}\\] The expected value of the prediction error given \\(Y^t\\) and \\(X^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, X^{t},Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_2(X^{t},Y^t,Z^t))]. \\end{equation}\\] In both models, the functions \\(f_1(.)\\) and \\(f_2(.)\\) are chosen to minimize the expected value of the loss function. In most cases, these functions are retrieved with linear and, possibly, with nonlinear regressions, neural networks etc. Typical forms for \\(g(.)\\) are the \\(l1\\)- or \\(l2\\)-norms. We can now provide our first definition of statistical causality under the Granger causal notion as follows: Definition 5.1 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(\\mathcal{R}(Y_{t+1} \\; | \\; X^t, Y^t, Z^t) = \\mathcal{R}(Y_{t+1} \\; | \\; Y^t, Z^t)\\). Standard Granger-causality tests assume a functional form in the relationship among the causes and effects and are implemented by fitting autoregressive models (Wiener 1956; Granger 1969). Consider the linear vector-autoregressive (VAR) equations: \\[\\begin{align} Y(t) &amp;= {\\alpha} + \\sum^k_{\\Delta t=1}{{\\beta}_{\\Delta t} Y(t-\\Delta t)} + \\epsilon_t, \\tag{5.1}\\\\ Y(t) &amp;= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} Y(t-\\Delta t)} + \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}X(t-\\Delta t)}+ \\widehat{\\epsilon}_t, \\tag{5.2} \\end{align}\\] where \\(k\\) is the number of lags considered. Alternatively, you can choose your DL/SVM/RF/GLM method of choice to fit the model. From Def. 5.1, \\(X\\) does not G-cause \\(Y\\) if and only if the prediction errors of \\(X\\) in the restricted Eq. (5.1) and unrestricted regression models Eq. (5.2) are equal (i.e., they are statistically indistinguishable). A one-way ANOVA test can be utilized to test if the residuals from Eqs. (5.1) and (5.2) differ from each other significantly. When more than one lag \\(k\\) is tested, a correction for multiple hypotheses testing should be applied, e.g. False Discovery Rate (FDR) or Bonferroni correction. 5.2 A Probabilistic-Based Definition A more general definition than Def. 5.1 that does not depend on assuming prediction functions can be formulated by considering conditional probabilities. A probabilistic definition of G-causality assumes that \\(Y_{t+1}\\) and \\(X^{t}\\) are independent given the past information \\((X^{t}, Y^{t})\\) if and only if \\(p(Y_{t+1} \\, | \\, X^{t}, Y^{t}, Z^{t}) = p(Y_{t+1} \\, | \\, Y^{t}, Z^{t})\\), where \\(p(. \\, | \\, .)\\) represents the conditional probability distribution. In other words, omitting past information from \\(X\\) does not change the probability distribution of \\(Y\\). This leads to our second definition of statistical causality as follows: Definition 5.2 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(Y_{t+1} {\\perp\\!\\!\\!\\!\\perp}X^{t} \\; | \\; Y^{t}, Z^{t}\\). Def. 5.2 does not assume any functional form in the coupling between \\(X\\) and \\(Y\\). Nevertheless, it requires a method to assess their conditional dependency. In the next section, we will leverage an Information-Theoretical framework for that purpose. 5.3 Transfer Entropy and Statistical Causality Given a coupled system \\((X,Y)\\), where \\(P_Y(y)\\) is the pdf of the random variable \\(Y\\) and \\(P_{X,Y}\\) is the joint pdf between \\(X\\) and \\(Y\\), the joint entropy between \\(X\\) and \\(Y\\) is given by the following: \\[\\begin{equation} H(X,Y) = -\\sum_{x \\in X}{\\sum_{y \\in Y}{P_{X,Y}(x,y)\\log{P_{X,Y}(x,y)}}}. \\label{eq:HXY} \\end{equation}\\] The conditional entropy is defined by the following: \\[\\begin{equation} H\\left(Y\\middle\\vert X\\right) = H(X,Y) - H(X). \\end{equation}\\] We can interpret \\(H\\left(Y\\middle\\vert X\\right)\\) as the uncertainty of \\(Y\\) given a realization of \\(X\\). Figure 5.3: Shannon, Claude. The concept of information entropy was introduced by Claude Shannon in his 1948 paper: A Mathematical Theory of Communication. To compute G-Causality, we use the concept of Transfer Entropy. Since its introduction (Schreiber 2000), Transfer Entropy has been recognized as an important tool in the analysis of causal relationships in nonlinear systems (Hlavackovaschindler et al. 2007). It detects directional and dynamical information (Montalto 2014) while not assuming any particular functional form to describe interactions among systems. The Transfer Entropy can be defined as the difference between the conditional entropies: \\[\\begin{equation} TE\\left(X \\rightarrow Y\\right \\vert Z) = H\\left(Y^F\\middle\\vert Y^P,Z^P\\right) - H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right), \\tag{5.3} \\end{equation}\\] which can be rewritten as a sum of Shannon entropies: \\[\\begin{align} TE\\left(X \\rightarrow Y\\right) = H\\left(Y^P, X^P\\right) - H\\left(Y^F, Y^P, X^P\\right) + H\\left(Y^F, Y^P\\right) - H\\left(Y^P\\right), \\end{align}\\] where \\(Y^F\\) is a forward time-shifted version of \\(Y\\) at lag \\(\\Delta t\\) relatively to the past time-series \\(X^P\\), \\(Y^P\\) and \\(Z^P\\). Within this framework we say that \\(X\\) does not G-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(H\\left(Y^F\\middle\\vert Y^P,Z^P \\right) = H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right)\\), i.e., when \\(TE\\left(X \\rightarrow Y,Z^P\\right) = 0\\). 5.4 Net Information Flow Transfer-entropy is an asymmetric measure, i.e., \\(T_{X \\rightarrow Y} \\neq T_{Y \\rightarrow X}\\), and it thus allows the quantification of the directional coupling between systems. The Net Information Flow is defined as \\[\\begin{equation} \\widehat{TE}_{X \\rightarrow Y} = TE_{X \\rightarrow Y} - TE_{Y \\rightarrow X}\\;. \\end{equation}\\] One can interpret this quantity as a measure of the dominant direction of the information flow. In other words, a positive result indicates a dominant information flow from \\(X\\) to \\(Y\\) compared to the other direction or, similarly, it indicates which system provides more predictive information about the other system (Michalowicz, Nichols, and Bucholtz 2013). 5.5 The Link Between Granger-causality and Transfer Entropy It has been shown (Barnett, Barrett, and Seth 2009) that linear G-causality and Transfer Entropy are equivalent if all processes are jointly Gaussian. In particular, by assuming the standard measure (\\(l2\\)-norm loss function) of linear G-causality for the bivariate case as follows (see Section 5.1 for more details on linear-Granger causality): \\[\\begin{equation} GC_{X \\rightarrow Y} = \\log\\left( \\frac{var(\\epsilon_t)}{var( \\widehat{\\epsilon}_t)} \\right), \\tag{5.4} \\end{equation}\\] the following can be proved (Barnett, Barrett, and Seth 2009): \\[\\begin{align} TE_{X \\rightarrow Y} = GC_{X \\rightarrow Y}/2. \\tag{5.5} \\end{align}\\] This result provides a direct mapping between the Transfer Entropy and the linear G-causality implemented in the standard VAR framework. Hence, it is possible to estimate the TE both in its general form and with its equivalent form for linear G-causality. 5.6 Information Flow on Simulated Systems In this section, we construct simulated systems to couple random variables in a causal manner. We then quantify information flow using the methods studied in this Chapter. We first assume a linear system, where random variables have linear relationships defined as follow: \\[\\begin{align} x_1(n) &amp;= 0.95\\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\\\ \\nonumber x_2(n) &amp;= 0.5x_1(n-1) + w_2\\\\ \\nonumber x_3(n) &amp;= -0.4x_1(n-1) + w_3\\\\ \\nonumber x_4(n) &amp;= -0.5x_1(n-1) + 0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_4\\\\ \\nonumber x_5(n) &amp;= -0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_5, \\nonumber \\end{align}\\] where \\(w_1, w_2, w_3, w_4, w_5 \\sim N(0, 1)\\). To simulate this system we assume \\(x_i(0) = 0, i \\in (1, 2, \\ldots, 5)\\) as initial condition and then iteratively generate \\(x_i\\) for \\(n \\in (1, 2, \\ldots, N)\\) with a total of \\(N = 10,000\\) iterations by randomly sampling \\(w_i, i \\in (1, 2, \\ldots, 5)\\) from a normal distribution with zero mean and unit variance. We simulate this linear system with the following code: set.seed(123) n &lt;- 10000 x1 &lt;- x2&lt;-x3&lt;-x4&lt;-x5&lt;-rep(0, n + 1) for (i in 2:(n + 1)) { x1[i] &lt;- 0.95 * sqrt(2)* x1[i - 1] -0.9025*x1[i - 1] + rnorm(1, mean=0, sd=1) x2[i] &lt;- 0.5*x1[i - 1] + rnorm(1, mean=0, sd=1) x3[i] &lt;- -0.4*x1[i - 1] + rnorm(1, mean=0, sd=1) x4[i] &lt;- -0.5*x1[i - 1] + 0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) x5[i] &lt;- -0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) } x1 &lt;- x1[-1] x2 &lt;- x2[-1] x3 &lt;- x3[-1] x4 &lt;- x4[-1] x5 &lt;- x5[-1] linear.system &lt;- data.frame(x1, x2, x3, x4, x5) The Fig. 5.4 represents the dependencies of the simulated linear system. Figure 5.4: Interactions between the variables of the simulated linear system. We first define a function that calculates a bi-variate measure for G-causality as defined in Eq. (5.4) as follows: Linear.GC &lt;- function(X, Y){ n&lt;-length(X) X.now&lt;-X[1:(n-1)] Y.now&lt;-Y[1:(n-1)] Y.fut&lt;-Y[2:n] regression.uni=lm(Y.fut~Y.now) regression.mult=lm(Y.fut~Y.now+ X.now) var.eps.uni &lt;- (summary(regression.uni)$sigma)^2 var.eps.mult &lt;- (summary(regression.mult)$sigma)^2 GC &lt;- log(var.eps.uni/var.eps.mult) return(GC) } We use the function calc_te from the package RTransferEntropy (Behrendt et al. 2019) and the previously defined function Linear.GC to calculate pairwise information flow among the simulated variables as follows: library(RTransferEntropy) library(future) ## Allow for parallel computing plan(multiprocess) ## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled ## in future (&gt;= 1.13.0) when running R from RStudio, because it is ## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall ## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to ## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more ## details, how to control forked processing or not, and how to silence this ## warning in future R sessions, see ?future::supportsMulticore ## Calculates GC and TE GC.matrix&lt;-FApply.Pairwise(linear.system, Linear.GC) TE.matrix&lt;-FApply.Pairwise(linear.system, calc_te) rownames(TE.matrix)&lt;-colnames(TE.matrix)&lt;-var.names&lt;-c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;) rownames(GC.matrix)&lt;-colnames(GC.matrix)&lt;-var.names ## Back to sequential plan(sequential) The function FApply.Pairwise is an auxiliary function that simply applies a given function D.Func to all possible pairs of columns from a given matrix X as follows: FApply.Pairwise &lt;- function(X, D.Func){ n = seq_len(ncol(X)) ff.TE.value = function(a, b) D.Func(X[,a], X[,b]) return(outer(n, n, Vectorize(ff.TE.value))) } Fig. 5.5 and Fig. 5.6 show Granger-causality and Transfer Entropy among the system’s variables, respectively. A cell \\((x, y)\\) presents the information flow from variable \\(y\\) to variable \\(x\\). We observe that both the Granger-causality (linear) and Transfer Entropy (nonlinear) approaches presented similar results, i.e., both methods captured the system’s dependencies similarly. This result is expected as the system is purely linear and Transfer Entropy is able to capture both the linear and nonlinear interactions. Figure 5.5: Granger-Causality of simulated linear system Figure 5.6: Transfer Entropy of simulated linear system We define a second system by introducing nonlinear interactions between \\(x_1\\) and the variables \\(x_2\\) and \\(x_5\\) as follows: \\[\\begin{align} x_1(n) &amp;= 0.95\\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\\\ \\nonumber x_2(n) &amp;= 0.5x_1^2(n-1) + w_2\\\\ \\nonumber x_3(n) &amp;= -0.4x_1(n-1) + w_3\\\\ \\nonumber x_4(n) &amp;= -0.5x_1^2(n-1) + 0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_4\\\\ \\nonumber x_5(n) &amp;= -0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_5, \\nonumber \\end{align}\\] where \\(w_1, w_2, w_3, w_4\\) and \\(w_5 \\sim N(0, 1)\\). To simulate this system we assume \\(x_i(0) = 0, i \\in (1, 2, ..., 5)\\) as initial condition and then iteratively generate \\(x_i\\) for \\(n \\in (1, 2, ..., N)\\) with a total of \\(N = 10,000\\) iterations by randomly sampling \\(w_i, i \\in (1, 2, ..., 5)\\) from a normal distribution with zero mean and unit variance. We simulate this nonlinear system with the following code: set.seed(123) n &lt;- 10000 x1 &lt;- x2&lt;-x3&lt;-x4&lt;-x5&lt;-rep(0, n + 1) for (i in 2:(n + 1)) { x1[i] &lt;- 0.95 * sqrt(2)* x1[i - 1] -0.9025*x1[i - 1] + rnorm(1, mean=0, sd=1) x2[i] &lt;- 0.5*x1[i - 1]^2 + rnorm(1, mean=0, sd=1) x3[i] &lt;- -0.4*x1[i - 1] + rnorm(1, mean=0, sd=1) x4[i] &lt;- -0.5*x1[i - 1]^2 + 0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) x5[i] &lt;- -0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) } x1 &lt;- x1[-1] x2 &lt;- x2[-1] x3 &lt;- x3[-1] x4 &lt;- x4[-1] x5 &lt;- x5[-1] nonlinear.system &lt;- data.frame(x1, x2, x3, x4, x5) Fig. 5.7 represents the dependencies of the simulated nonlinear system. Figure 5.7: Interactions between the variables of the simulated nonlinear system. We calculate Granger-causality and Transfer Entropy of the simulated nonlinear system as follows: ## Allow for parallel computing plan(multiprocess) ## Calculates GC and TE GC.matrix.nonlinear&lt;-FApply.Pairwise(nonlinear.system, Linear.GC) TE.matrix.nonlinear&lt;-FApply.Pairwise(nonlinear.system, calc_te) rownames(TE.matrix.nonlinear)&lt;-colnames(TE.matrix.nonlinear)&lt;-var.names rownames(GC.matrix.nonlinear)&lt;-colnames(GC.matrix.nonlinear)&lt;-var.names ## Back to sequential computing plan(sequential) From Fig. 5.8 and Fig. 5.9, we observe that the nonlinear interactions introduced were not captured by the linear form of the information flow. While all linear interactions presented similar linear and nonlinear information flows, the two nonlinear interactions introduced in the system presented relatively higher nonlinear information flow compared to the linear formulation. Figure 5.8: Granger-Causality of simulated nonlinear system Figure 5.9: Transfer Entropy of simulated nonlinear system 5.7 Information Flow among International Stock Market Indices The world’s financial markets form a complex, dynamic network in which individual markets interact with one another. This multitude of interactions can lead to highly significant and unexpected effects, and it is vital to understand precisely how various markets around the world influence one another (Junior, Mullokandov, and Kenett 2015). In this section, we use Transfer Entropy for the identification of dependency relations among international stock market indices. First, we select some of the major global indices for our analysis, namely the S&amp;P 500, the FTSE 100, the DAX, the EURONEXT 100 and the IBOVESPA, which track the following markets, respectively, the US, the UK, Germany, Europe and Brazil. They are defined by the following tickers: tickers&lt;-c(&quot;^GSPC&quot;, &quot;^FTSE&quot;, &quot;^GDAXI&quot;, &quot;^N100&quot;, &quot;^BVSP&quot;) Next, we will load log-returns of daily closing adjusted prices for the selected indices as follows (see Appendix B.1 for code used to generate this dataset): library(xts) dataset&lt;-as.xts(read.zoo(&#39;./data/global_indices_returns.csv&#39;, header=TRUE, index.column=1, sep=&quot;,&quot;)) head(dataset) ## X.GSPC X.FTSE X.GDAXI X.N100 X.BVSP ## 2000-01-02 19:00:00 0.000000 NA 0.00000 0.00000 0.00000 ## 2000-01-03 19:00:00 -0.039099 0.00000 -0.02456 -0.04179 -0.06585 ## 2000-01-04 19:00:00 0.001920 -0.01969 -0.01297 -0.02726 0.02455 ## 2000-01-05 19:00:00 0.000955 -0.01366 -0.00418 -0.00842 -0.00853 ## 2000-01-06 19:00:00 0.026730 0.00889 0.04618 0.02296 0.01246 ## 2000-01-09 19:00:00 0.011128 0.01570 0.02109 0.01716 0.04279 The influence that one market plays in another is dynamic. Here, we will consider the time period from 01/01/2014 until today and we will omit days with invalid returns due to bad data using the function NARV.omit from the package IDPmisc as follows: library(IDPmisc) dataset.post.crisis &lt;- NaRV.omit(as.data.frame(dataset[&quot;2014-01-01/&quot;])) We will calculate pairwise Transfer Entropy among all indices considered and construct a matrix such that each value in the position \\((i,j)\\) will contain the value Transfer Entropy from \\(tickers[i]\\) to \\(tickers[j]\\) as follows: ## Allow for parallel computing plan(multiprocess) # Calculate pairwise Transfer Entropy among global indices TE.matrix&lt;-FApply.Pairwise(dataset.post.crisis, calc_ete) rownames(TE.matrix)&lt;-colnames(TE.matrix)&lt;-tickers ## Back to sequential computing plan(sequential) Fig. 5.10 displays the resulting Transfer Entropy matrix. We normalize the Transfer Entropy values by dividing it by the maximum value in the matrix such that all values range from 0 to 1. We observe that the international indices studied are highly interconnected in the period analyzed with the highest information flow going from the US market to the UK market (^GSPC -&gt; ^FTSE). The second highest information flow is going from the UK market to the US market. That’s a result we would expect as the US and the UK markets are strongly coupled, historically. Figure 5.10: Normalized Transfer Entropy among international stock market indices. We also calculate the marginal contribution of each market to the total Transfer Entropy in the system by calculating the sum of Transfer Entropy for each row in the Transfer Entropy matrix, which we also normalize such that all values range from 0 to 1: TE.marginal&lt;-base::apply(TE.matrix, 1, sum) TE.marginal.norm&lt;-TE.marginal/sum(TE.marginal) print(TE.marginal.norm) ## ^GSPC ^FTSE ^GDAXI ^N100 ^BVSP ## 0.346 0.215 0.187 0.117 0.135 We observe that the US is the most influential market in the time period studied detaining 34.632% of the total Transfer Entropy followed by the UK and Germany with 21.472% and 18.653%, respectively. Japan and Brazil are the least influential markets with normalized Transfer Entropies of 11.744% and 13.498%, respectively. An experiment left to the reader is to build a daily trading strategy that exploits information flow among international markets. The proposed thesis is that one could build a profitable strategy by placing bets on futures of market indices that receive significant information flow from markets that observed unexpected returns/movements. For an extended analysis with a broader set of indices see (Junior, Mullokandov, and Kenett 2015). The authors develop networks of international stock market indices using an information theoretical framework. They use 83 stock market indices of a diversity of countries, as well as their single day lagged values, to probe the correlation and the flow of information from one stock index to another taking into account different operating hours. They find that Transfer Entropy is an effective way to quantify the flow of information between indices, and that a high degree of information flow between indices lagged by one day coincides to same day correlation between them. 5.8 Other Applications 5.8.1 Quantifying Information Flow Between Social Media and the Stock Market Investors’ decisions are modulated not only by companies’ fundamentals but also by personal beliefs, peers influence and information generated from news and the Internet. Rational and irrational investor’s behavior and their relation with the market efficiency hypothesis (Fama 1970) have been largely debated in the economics and financial literature (Shleifer 2000). However, it was only recently that the availability of vast amounts of data from online systems paved the way for the large-scale investigation of investor’s collective behavior in financial markets. A research paper (Souza and Aste 2016) used some of the methods studied in this Chapter to uncover that information flows from social media to stock markets revealing that tweets are causing market movements through a nonlinear complex interaction. The authors provide empirical evidence that suggests social media and stock markets have a nonlinear causal relationship. They take advantage of an extensive data set composed of social media messages related to DJIA index components. By using information-theoretic measures to cope for possible nonlinear causal effects between social media and the stock market, the work points out stunning differences in the results with respect to linear coupling. Two main conclusions are drawn: First, social media significant causality on stocks’ returns are purely nonlinear in most cases; Second, social media dominates the directional coupling with stock market, an effect not observable within linear modeling. Results also serve as empirical guidance on model adequacy in the investigation of sociotechnical and financial systems. Fig. 5.11 shows the significant causality links found between social media and stocks’ returns considering both cases: nonlinear (Transfer Entropy) and linear G-causality (linear VAR framework). The linear analysis discovers only three stocks with significant causality: INTEL CORP., NIKE INC. and WALT DISNEY CO. The Nonlinear analysis discovers that several other stocks have significant causality. In addition to the 3 stocks identified with significant linear causality, other 8 stocks presented purely nonlinear causality. Figure 5.11: Demonstration that the causality between social media and stocks’ returns are mostly nonlinear. Linear causality test indicated that social media caused stock’s returns only for 3 stocks. Nonparametric analysis showed that almost 1/3 of the stocks rejected in the linear case have significant nonlinear causality. In the nonlinear case, Transfer Entropy was used to quantify causal inference between the systems with randomized permutations test for significance estimation. In the linear case, a standard linear G-causality test was performed with a F-test under a linear vector-autoregressive framework. A significant linear G-causality was accepted if its linear specification was not rejected by the BDS test. p-values are adjusted with the Bonferroni correction. Significance is given at p-value &lt; 0.05. The low level of causality obtained under linear constraints is in-line with results from similar studies in the literature, where it was found that stocks’ returns show weak causality links (Alanyali, Moat, and Preis 2013, Antweiler and Frank (2004)) and social media sentiment analytics, at least when taken alone, have small or no predictive power (Ranco 2015) and do not have significant lead-time information about stock’s movements for the majority of the stocks (Zheludev, Smith, and Aste 2014). Contrariwise, results from the nonlinear analyses unveiled a much higher level of causality indicating that linear constraints may be neglecting the relationship between social media and stock markets. In summary, this paper (Souza and Aste 2016) is a good example on how causality can be not only complex but also misleading further highlighting the importance of choice in the methodology used to quantify it. 5.8.2 Detecting Causal Links Between Investor Sentiment and Cryptocurrency Prices In (Keskin and Aste 2019), the authors use information-theoretic measures studied in this Chapter for non-linear causality detection applied to social media sentiment and cryptocurrency prices. Using these techniques on sentiment and price data over a 48-month period to August 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple (XRP), litecoin (LTC) and ethereum (ETH), the authors detect significant information transfer, on hourly timescales, in directions of both sentiment to price and of price to sentiment. The work reports the scale of non-linear causality to be an order of magnitude greater than linear causality. The information-theoretic investigation detected a significant non-linear causal relationship in BTC, LTC and XRP, over multiple timescales and in both the directions sentiment to price and price to sentiment. The effect was strongest and most consistent for BTC and LTC. Fig. 5.12 shows Transfer Entropy results between BTC sentiment and BTC prices. Figure 5.12: Evidence that BTC sentiment and price are causally coupled in both directions in a non-linear way. Non-linear TE is calculated by multidimensional histograms with 6 quantile bins per dimension. Z-scores, calculated over 50 shuffles, show a high level of significance, especially during 2017 and 2018, in both directions. All analysis for this paper was performed using a Python package (PyCausality), which is available at https://github.com/ZacKeskin/PyCausality. 5.9 Conclusions Untangling cause and effect can be devilishly difficult. However, statistical tools can help us tell correlation from causation. In this Chapter, we introduced the notion of Granger-causality and its traditional implementation in a linear vector-autoregressive framework. We then defined information theoretical measures to quantify Transfer Entropy as a method to estimate statistical causality in nonlinear systems. We simulated linear and nonlinear systems further showing that the traditional linear G-causality approach failed to detect simple non-linearities introduced into the system while Transfer Entropy successfully detected such relationships. Finally, we showed how Transfer Entropy can be used to quantify relationships among global equity indexes. We also discussed further applications from the literature where Information Theoretical measures were used to quantify causal links between investor sentiment and movements in the equity and crypto markets. We hope you enjoyed this casual causal journey and remember: Quantify causality, responsibly. References "],
["financial-networks.html", "Chapter 6 Financial Networks 6.1 Introduction 6.2 Network Construction 6.3 Applications", " Chapter 6 Financial Networks 6.1 Introduction Financial markets can be regarded as a complex network in which nodes represent different financial assets and edges represent one or many types of relationships among those assets. Filtered correlation-based networks have successfully been used in the literature to study financial markets structure particularly from observational data derived from empirical financial time series (Bardoscia et al. 2017; S. A. L. Tumminello Michele AND Miccichè 2011; R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010, M. Tumminello et al. (2005)). The underlying principle is to use correlations from empirical financial time series to construct a sparse network representing the most relevant connections. Analyses on filtered correlation-based networks for information extraction (Song, Aste, and Di Matteo 2008; T. Aste, Shaw, and Di Matteo 2010) have widely been used to explain market interconnectedness from high-dimensional data. Applications include asset allocation (Y. Li et al. 2018; Pozzi, Di Matteo, and Aste 2013), market stability assessments (Morales et al. 2012), hierarchical structure analyses (R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010; Musmeci, Aste, and Matteo 2014; Song, Di Matteo, and Aste 2012) and the identification of lead-lag relationships (Curme, Stanley, and Vodenska 2015). In this Chapter we will describe how to Construct and filter financial networks; Build price-based dynamic industry taxonomies; Implement a trading strategy based on financial network structure. 6.2 Network Construction We selected \\(N = 100\\) of the most capitalized companies that were part of the S&amp;P500 index from 09/05/2012 to 08/25/2017. The list of these companies’ ticker symbols is reported in the Appendix . For each stock \\(i\\) the financial variable was defined as the daily stock’s log-return \\(R_i(\\tau)\\) at time \\(\\tau\\). Stock returns \\(R_i\\) and social media opinion scores \\(O_i\\) each amounted to a time series of length equals to 1251 trading days. These series were divided time-wise into \\(M = 225\\) windows \\(t = 1, 2, \\ldots, M\\) of width \\(T = 126\\) trading days. A window step length parameter of \\(\\delta T = 5\\) trading days defined the displacement of the window, i.e., the number of trading days between two consecutive windows. The choice of window width \\(T\\) and window step \\(\\delta T\\) is arbitrary, and it is a trade-off between having analysis that is either too dynamic or too smooth. The smaller the window width and the larger the window steps, the more dynamic the data are. To characterize the synchronous time evolution of assets, we used equal time Kendall’s rank coefficients between assets \\(i\\) and \\(j\\), defined as \\[\\begin{equation} \\rho_{i, j}(t) = \\sum\\limits_{t&#39; &lt; \\tau}sgn(V_i(t&#39;) - V_i(\\tau))sgn(V_j(t&#39;) - V_j(\\tau)), \\end{equation}\\] where \\(t&#39;\\) and \\(\\tau\\) are time indexes within the window \\(t\\) and \\(V_i \\in \\{R_i, O_i\\}\\). Kendall’s rank coefficients takes into account possible nonlinear (monotonic) relationships. It fulfill the condition \\(-1 \\leq \\rho_{i, j} \\leq 1\\) and form the \\(N \\times N\\) correlation matrix \\(C(t)\\) that served as the basis for the networks constructed in this work. To construct the asset-based financial and social networks, we defined a distance between a pair of stocks. This distance was associated with the edge connecting the stocks, and it reflected the level at which they were correlated. We used a simple non-linear transformation \\(d_{i, j}(t) = \\sqrt{2(1 - \\rho_{i,j}(t))}\\) to obtain distances with the property \\(2 \\geq d_{i,j} \\geq 0\\), forming a \\(N \\times N\\) symmetric distance matrix \\(D(t)\\). 6.2.1 Network Filtering: Asset Graphs We extract the \\(N(N-1)/2\\) distinct distance elements from the upper triangular part of the distance matrix \\(D(t)\\), which were then sorted in an ascending order to form an ordered sequence \\(d_1(t), d_2(t), \\ldots, d_{N(N-1)/2}(t)\\). Since we require the graph to be representative of the market, it is natural to build the network by including only the strongest connections. This is a network filtering procedure that has been successfully applied in the construction of for the analyses of market structure . The number of edges to include is arbitrary, and we included those from the bottom quartile, which represented the 25% shortest edges in the graph (largest correlations), thus giving \\(E(t) = \\{d_1(t), d_2(t), \\ldots, d_{\\floor{N/4}}(t)\\}\\). We denoted \\(E^{F}(t)\\) as the set of edges constructed from the distance matrix derived from stock returns \\(R(t)\\). The financial network considered is \\(G^{F} = ( V, E^{F} )\\), where \\(V\\) is the vertex set of stocks. 6.2.2 Network Filtering: MST 6.2.3 Network Filtering: PMFG 6.3 Applications 6.3.1 Industry Taxonomy 6.3.2 Portfolio Construction References "],
["the-market-the-players-and-the-rules.html", "Chapter 7 The Market, The Players and The Rules 7.1 The Market 7.2 The Data 7.3 The Buyers 7.4 Conclusion", " Chapter 7 The Market, The Players and The Rules 7.1 The Market In an industry where almost every player has access to the same (or similar) core financial and fundamental data, investment firms look for Alternative Data seeking differentiated insights into companies not found in filings, earnings calls or fundamental datasets. Until recently, the usage of alternative data has been confined mainly to the realm of quantitative investment managers, as these firms were best suited to obtain, clean and process this data. Now, however, alternative data is beginning to go mainstream, with increasing interest from fundamental and hybrid asset managers here (Johnson 2019). Alternative Data is Untapped Alpha. The biggest opportunity for investors in this decade comes from the signals buried in the data generated by the digital economy. Alternative data is the deepest, least utilized alpha source in the world today - Quandl. On June/2019, alternative data went mainstream when Verizon made a bold move by launching a subscription service for Yahoo Finance that offers alternative data and insights to retail investors at a price point of $34.99/month. The amount spent in Alternative Data have been steadily increasing. More than half of all quantitative and fundamental investors surveyed recently are either considering or using alternative data as part of their workflow - Greenwich Associates. A recent Greenwich Associates report found that a wide majority of investment managers, 72%, stated that alternative data was improving enhanced their signal quality in an arena where filtering out signal noise. Of those who are implementing an alternative data strategy, more than one-fifth claim to have received 20% or more of their alpha from the practice. Figure 7.1: Total Buy-side spend on Alternative Data has been steadily increasing and its likely to nearly triple from 2018 to 2020. 7.2 The Data But where’s Alternative Data coming from and who are the key players providing access to it? Figure 7.2: Alternative Data are sourced by heterogeneous sources including individuals, businesses and sensors. The growth of Alternative Data has been enabled by the digitalization of the world around us. Data have been produced at an unprecedented rate by heterogeneous sources including: Individuals who today mirror their lives in the form digital behavior into the Web, Social Media and Apps; Sensors, particularly with the emergence of IoT, that generate satellite images, weather forecasts and geolocation data; Businesses from which data is generated in the form of traditional official filings and earning calls as well as banking records or supply chain data. Figure 7.3: The number of Alternative Data Providers is exploding. The number of Alternative Data Providers is exploding.The number of alternative data vendors is exploding particularly in the past decade. A few companies are worth mentioning: Quandl is likely the largest financial and alternative data aggregator/provider today. They leverage relationships with third-party providers to be a one-stop-shop for alternative data from consumers, IoT and sentiment to traditional fundamental, pricing and estimates. Quandl has been recently acquired by Nasdaq. Dataminr leverages advanced anomaly detection technology in a real-time AI platform to uncover critical events from social media before the information goes mainstream. The company raised a staggering $500MM+ amount in funding so far and it gives no sign of that is slowing down. Thinknum provides a SaaS-based web platform for a variety of alternative data sets, in particular, web-scrapped data such as product comparisons and company ratings on jobs sites. Thinkum claims to have 8 of the 10 largest investment banks as their clients. They recently closed a $11.6 million Series A funding to expand their financial modeling tools. Yewno is helping the world to uncover the undiscovered through its advanced dynamic Knowledge Graph and AI based inference engine, which introduces an entirely new approach to knowledge extraction to enhance human understanding by correlating concepts across a vast volume of sources. Yewno ingests data from sources such as clinical trials, patents, company transcripts and court opinions and more. Yewno raised about $30 million dollars in funding and it has launched AI-based equity indexes with major partners such as STOXX and Nasdaq and licensed alternative data feeds to buy-side firms. Other rising vendors include YipitData, 1010data and Enigma as well as incumbent players such as Refinitiv, S&amp;P Global Market Intelligence, Bloomberg and Factset. While the alternative data offering is exploding there is a gap between what data vendors offer and what data buyers want. A research report conducted by BattleFin and AlternativeData.org has found that there is a gap between what data vendors offer and what data buyers actually want: - Data buyers view the three most-valuable data categories as: credit/debit cards (14.83%) web data (12.29%), and social/sentiment (11.02%). - By contrast, the percentage of data providers covering these data categories are: credit card (0.69%), web-crawled data (3.94%) and sentiment (4.40%). - Data providers report that the top-three categories their data covers are: business insights (6.48%), web traffic (4.63%) and sentiment (4.40%). Figure 7.4: Data extracted from a research report conducted by BattleFin and AlternativeData.org, which compiles responses from 173 respondents made up of 69 alternative-data buyers and 104 data providers who responded to a 2018 survey conducted by BattleFin. 7.3 The Buyers The data buyers include your usual suspects such as Two Sigma, Milennium, Third Point, WorldQuant, ExodusPoint and emerging Quant Funds like Credit Suisse’s QT Fund. 80% of buyers expect to purchase a minimum of one to five datasets in 2019 while nearly 60 percent expects to test and evaluate a minimum of one to five datasets this year - according to a research report conducted by BattleFin and AlternativeData.org. Pension Plans and Index Providers are also entering the space seeking exposure to alternative factors, particularly with the emergence of AI-based strategies that leverage machine learning and computational linguistics techniques to extract signals from unstructured data (see STOXX’s or DWS’s AI-based indexes) as well as the increased interest in themes such as ESG (Environmental, Social and Corporate Governance) - a multi-dimensional multi-sourced theme with increasing demands that are no longer met by traditional data sources alone. I have been to a good number of meetings with data buyers (BattleFin is maybe the best event out there for 1-on-1 meetings with high-profile buy-side firms sometimes allowing for 30+ meetings in two days of events in addition to dozens of unofficial ones). Some questions are commonplace: Can you describe your data sources and your data ingestion process? What’s the frequency/history/coverage? What’s the delivery method? What are the key use cases for your data? Can I run a trial with live data? Who are your clients? What’s pricing like? Do you offer exclusivity? What is the size/funding/team profile of your firm? There is one tricky question though: Whether the data was already licensed to other clients. Particularly if the buyer is looking for alpha, they actually would prefer to be one of the first to test the data and avoid using a dataset that is already “crowded”. By the same token, they would be happy to hear that the dataset was already licensed to reputable clients as a sign of credibility. It’s up to you to navigate through this trade-off. Figure 7.5: The sales life-cycle of an alternative data product can take up to seven months from lead generation to customer sign-up. Image by Quandl. This is just the very first step in the sales and marketing life-cycle. From lead generation to customer sign-up, data buyers can take several months to make a final decision. Here are some findings: Providers and buyers agree that alt-data sales cycle can take up to seven months, with more than 75% of buyers requiring up to six months to test, evaluate and purchase an alt dataset. More than a third of data buyers use backtesting to inform their purchasing decision. Nearly 60% of providers are not interested in outsourcing the data testing process even if it can be handled efficiently. More than 20% of providers sell and distribute using a customized alt-data software platform. Nearly 18% use no software platform at all. 7.4 Conclusion Alternative Data, once a treat only for the most advanced Hedge Funds, is becoming a must-have in the alpha-generation war on Wall Street. Total spending in the area has been steadily growing as well as the number of data vendors and variety of data products. Relevant acquisitions have been made in the space and the first unicorn startups have been formed. However, there are critical challenges in the industry. There is a gap between what buyers find most relevant and what the data vendors are producing. Further, the sales life-cycle is long, as players are still figuring out how to test the value of the data and considerable time is spent on on-boarding new datasets. Successful players will be those who bridge the gap between data and insights first. The race has just begun, and it looks like no one wants to be just a spectator. References "],
["statistical-methods.html", "A Statistical Methods A.1 Kernel Density Estimation", " A Statistical Methods This Appendix provides details to some of statistical methods used in the book. A.1 Kernel Density Estimation In the entropy computation (see Section 4) the empirical probability distribution must be estimated. Histogram-based methods and kernel density estimations are the two main methods for that. Histogram-based is the simplest and most used nonparametric density estimator. Nonetheless, it yields density estimates that have discontinuities and vary significantly depending on the bin size choice. Also known as the Parzen-Rosenblatt window method, the kernel density estimation (KDE) approach approximates the density function at point \\(x\\) using neighboring observations. However, instead of building up the estimate according to the bin edges as in histograms, the KDE method uses each point of estimation \\(x\\) as the center of a bin of width \\(2h\\) and weight it according to a kernel function. Thereby, the kernel estimate of the probability density function \\(f(x)\\) is defined as \\[\\begin{equation} \\hat{f} = \\frac{1}{nh}\\sum_{x&#39; \\in X}{K\\left(\\frac{x - x&#39;}{h}\\right)}. \\tag{A.1} \\end{equation}\\] A usual choice for the kernel \\(K\\), which we use here, is the (Gaussian) radial basis function: \\[\\begin{equation} K(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}x^2}. \\end{equation}\\] The problem of selecting the bandwidth \\(h\\) in Eq. (A.1) is crucial in the density estimation. A large \\(h\\) will oversmooth the estimated density and mask the structure of the data. On the other hand, a small bandwidth will reduce the bias of the density estimate at the expense of a larger variance in the estimates. If we assume that the true distribution is Gaussian and we use a Gaussian kernel, the optimal value of \\(h\\) that minimizes the mean integrated squared error (MISE) is \\[\\begin{equation*} h^* = 1.06\\sigma N^{-1/5}, \\end{equation*}\\] where \\(N\\) is the total number of points and \\(\\sigma\\) can be estimated as the sample standard deviation. This bandwidth estimation is often called the Gaussian approximation or Silverman’s rule of thumb for kernel density estimation (Silverman and Green 1986). This is the most commonly-used method and it is here employed. Other common methods are given by (Sheather and Jones 1991) and (Scott 1992). References "],
["datasets.html", "B Datasets B.1 Log-Returns of International Stock Market Indices Prices B.2 Log-Returns of FAANG Prices", " B Datasets This Chapter provides code for datasets produced for the book. B.1 Log-Returns of International Stock Market Indices Prices B.1.1 Dataset Location ./data/global_indices_returns.csv B.1.2 Dataset Description Log-returns of adjusted prices for the indices identified by the following tickers: ^GSPC, ^FTSE, ^GDAXI, ^N100 and ^BVSP. B.1.3 Data Source Alpha Vantage B.1.4 Code GetReturn &lt;- function(tickers){ library(quantmod) data.env &lt;- new.env() dataset&lt;- xts() # Only run once # Download prices from AlphaVantage and calculate log-returns for(i in 1:length(tickers)) { tickers[i]-&gt; symbol print(symbol) getSymbols(symbol, src=&quot;av&quot;, auto.assign=TRUE, output.size=&quot;full&quot;, adjusted=TRUE, api.key=config::get()$alpha.vantage.key) dataset &lt;- merge(dataset, periodReturn(Ad(get(tickers[i])),period=&quot;daily&quot;, type=&#39;log&#39;)) rm(symbol) } names(dataset)&lt;-tickers return(dataset) } tickers&lt;-c(&quot;^GSPC&quot;, &quot;^FTSE&quot;, &quot;^GDAXI&quot;, &quot;^N100&quot;, &quot;^BVSP&quot;) dataset&lt;-GetReturn(tickers) tmp &lt;- tempfile() write.zoo(dataset,sep=&quot;,&quot;,file=&quot;./data/global_indices_returns.csv&quot;) B.1.5 Dataset Scheme library(xts) dataset&lt;-as.xts(read.zoo(&#39;./data/global_indices_returns.csv&#39;, header=TRUE, index.column=1, sep=&quot;,&quot;)) tail(dataset) ## X.GSPC X.FTSE X.GDAXI X.N100 X.BVSP ## 2019-08-08 20:00:00 -0.00664 -0.00440 -0.01288 -0.01114 -0.00114 ## 2019-08-11 20:00:00 -0.01239 -0.00376 -0.00121 -0.00342 -0.02021 ## 2019-08-12 20:00:00 0.01502 0.00334 0.00601 0.00661 0.01349 ## 2019-08-13 20:00:00 -0.02973 -0.01431 -0.02216 -0.01956 -0.02988 ## 2019-08-14 20:00:00 0.00246 -0.01138 -0.00698 -0.00380 -0.01205 ## 2019-08-15 20:00:00 0.01432 0.00708 0.01306 0.01340 0.00753 B.2 Log-Returns of FAANG Prices B.2.1 Dataset Location ./data/FAANG.csv B.2.2 Dataset Description Log-returns of adjusted prices for the stocks identified by the following tickers: FB, AMZN, AAPL, NFLX and GOOG. B.2.3 Data Source Alpha Vantage B.2.4 Code tickers&lt;-c(&quot;FB&quot;, &quot;AMZN&quot;, &quot;AAPL&quot;, &quot;NFLX&quot;, &quot;GOOGL&quot;) dataset&lt;-GetReturn(tickers) tmp &lt;- tempfile() write.zoo(dataset,sep=&quot;,&quot;,file=&quot;./data/FAANG.csv&quot;) B.2.5 Dataset Scheme dataset&lt;-as.xts(read.zoo(&#39;./data/FAANG.csv&#39;, header=TRUE, index.column=1, sep=&quot;,&quot;)) tail(dataset) ## FB AMZN AAPL NFLX GOOG ## 2019-08-22 20:00:00 -0.023848 -0.03097 -0.04732 -0.01866 -0.032675 ## 2019-08-25 20:00:00 0.014577 0.01094 0.01882 0.01207 0.015172 ## 2019-08-26 20:00:00 0.005198 -0.00399 -0.01135 -0.01348 -0.000899 ## 2019-08-27 20:00:00 0.002534 0.00137 0.00669 0.00254 0.002719 ## 2019-08-28 20:00:00 0.020745 0.01248 0.01679 0.01703 0.018470 ## 2019-08-29 20:00:00 0.000539 -0.00568 -0.00129 -0.01026 -0.003990 "],
["references.html", "References", " References "]
]
