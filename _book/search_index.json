[
["index.html", "The Open Quant Live Book Preface", " The Open Quant Live Book by OpenQuants.com 2019-08-06 Preface Description The book aims to be an Open Source introductory reference of the most important aspects of financial data analysis, algo trading, portfolio selection, econophysics and machine learning in finance with an emphasis in reproducibility and openness not to be found in most other typical Wall Street-like references. Contribute The Book is Open and we are looking for co-authors. Feel free to reach out or simply create a pull request with your contribution! See project structure, guidelines and how to contribute here. Working Contents The Basics I/O Stylized Facts Algo Trading Investment Process Backtesting Factor Investing Limit Order Portfolio Optimization Convex Optimization Risk Parity Portfolios Machine Learning Intro Agent-Based Models Binary Classifiers AutoML Hierarchical Risk Parity Econophysics Entropy, Efficiency and Bubbles Nonparametric Statistical Causality: An Information-Theoretical Approach Financial Networks Alternative Data The Market, The Players, The Rules Case Studies Book’s information First published at: openquants.com. Licensed under Attribution-NonCommercial-ShareAlike 4.0 International. Copyright (c) 2019. OpenQuants.com, New York, NY. "],
["io.html", "Chapter 1 I/O 1.1 Importing Data 1.2 Data Sources 1.3 Conclusion", " Chapter 1 I/O In this Chapter, we will introduce basic functions to read text, excel and JSON files as well as large files. We will also show how to obtain free financial and economic data including the following: End-of-day and real-time pricing; Company financials; Macroeconomic data. Data sources utilized in this Chapter include the following: U.S. Securities and Exchange Commission; Quandl; IEX; Alpha Vantage. 1.1 Importing Data 1.1.1 Text Files The most basic and commonly used option to import data from text files in R is the use of the function read.table from the r-base. We can use this function to read text files with extensions such as .txt and .csv. dat.table &lt;- read.table(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- read.csv(file = &quot;&lt;name of your file&gt;.csv&quot;) The package readr provides functions for reading text data into R that are much faster that the functions from the r-base. The read_table function from the package readr provides a near-replacement for the read.table function. library(readr) dat.table &lt;- readr::read_table2(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- readr::read_csv(file = &quot;&lt;name of your file&gt;.csv&quot;) Another option to save data is to write it in rds format. Data stored in rds format has the advantage to keep the original data struture and type of the object saved. Also, .rds files are compressed and consume less space than files saved in .csv format. A data.frame object can be saved in rds format and then loaded back as follows: write_rds(dat.frame, path = &quot;&lt;name of your file&gt;.rds&quot;) dat.frame &lt;- read_rds(path = &quot;&lt;name of your file&gt;.rds&quot;) 1.1.2 Excel Files The package readxl has an ease to use interface to functions that load excel documents in R. The functions read_xls and read_xlsx can be used to read excel files as follows: library(readxl) readxl::read_xls(path = &quot;&lt;name of your file&gt;.xls&quot;) readxl::read_xlsx(path = &quot;&lt;name of your file&gt;.xlsx&quot;) The function read_excel() automatically detects the extension of the input file as follows: readxl::read_excel(&quot;&lt;name and extension of your file&gt;&quot;, sheet = &quot;&lt;sheet name or index&gt;&quot;) In the read_excel function, the sheet argument can receive either the target sheet name or index number, where sheet indexing starts at 1. The readxl has been oberving increased use compared to other comparable packages such as gdata and the xlsx due to its relative ease of use and performance. Also, the readxl do not have depency with external code libraries while the packages gdata and xlsx depend on ActiveState PERL and the Java JDK, respectively. 1.1.3 JSON Files JSON files are particularly used for transmitting data in web applications but also frequently used as a standard data interchange format. The jsonline package can be used to parse files in JSON format as follows: library(jsonlite) result_json &lt;- read_json(&quot;&lt;json file&gt;&quot;) 1.1.4 Large Files Fast data manipulation in a short and flexible syntax. 1.2 Data Sources In this section, we will show how to obtain financial and economic data from public sources. 1.2.1 Alpha Vantage Alpha Vantage offers free access to pricing data including: Stock Time Series Data; Physical and Digital/Crypto Currencies (e.g., Bitcoin); Technical Indicators and Sector Performances. The data are available in JSON and CSV formats via REST APIs. The quantmod and the alphavantager R packages offer a lightweight R interface to the Alpha Vantage API. Daily stock prices can be obtained with the quantmod::getSymbols function as follows: getSymbols(Symbols=&#39;AAPL&#39;, src=&quot;av&quot;, output.size=&quot;full&quot;, adjusted=TRUE, api.key=&#39;your API key&#39;) The output data is stored in an object with the same name as the corresponding symbol, in this example AAPL. The output data looks like the following AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted 13.6 16.2 13.5 16.2 6411700 0.510 16.5 16.6 15.2 15.9 5820300 0.499 15.9 20.0 14.8 18.9 16182800 0.595 18.8 19.0 17.3 17.5 9300200 0.550 17.4 18.6 16.9 18.2 6910900 0.571 18.1 19.4 17.5 18.2 7915600 0.571 We called the quantmod::getSymbols function with the following arguments: Symbols='AAPL' defines a character vector specifying the names of each symbol to be loaded, here specified by the symbol of the company Apple Inc.; src=&quot;av&quot; specifies the sourcing method, here defined with the value corresponding to Alpha Vantage; output.size=&quot;full&quot;specified length of the time series returned. The strings compact and full are accepted with the following specifications: compact returns only the latest 100 data points; full returns the full-length time series of up to 20 years of historical data; adjusted=TRUE defines a boolean variable to include a column of closing prices adjusted for dividends and splits; api.key specifies your Alpha Vantage API key. 1.2.2 IEX The IEX Group operates the Investors Exchange (IEX), a stock exchange for U.S. equities that is built for investors and companies. IEX offers U.S. reference and market data including end-of-day and intraday pricing data. IEX offers an API with “a set of services designed for developers and engineers. It can be used to build high-quality apps and services”. Data sourced from the IEX API is freely available for commercial subject to conditions and the use of their API is subject to additional terms of use. IEX lists the following github project as an unofficial API for R: https://github.com/imanuelcostigan/iex. We will provide examples on how to obtain intraday pricing data using this package. First, we will use the devtools to install the package directly from its github repository as follows: library(devtools) install_github(&quot;imanuelcostigan/iex&quot;) The iex package provides 4 set of functions as follows: last: Provides IEX near real time last sale price, size and time. Last is ideal for developers that need a lightweight stock quote. IEX API real time API documentation. market: Provides exchange trade volume data in near real time. IEX market API documentation. stats: A set of functions that return trading statistics. IEX stats API documentation. tops: Provides IEX’s aggregated bid and offer position in near real time for all securities on IEX’s displayed limit order book. IEX API TOPS documentation. For instance, the last function has the following arguments: symbols: A vector of tickers (case insensitive). Special characters will be escaped. A list of eligible symbols is published daily by the IEX. When set to NULL (default) returns values for all symbols. fields: A vector of fields names to return (case sensitive). When set to NULL (default) returns values for all fields. version: The API version number, which is used to define the API URL. We can obtain intraday stock price data with the last function as follows: dat &lt;- iex::last(symbols = c(&quot;AAPL&quot;), fields = c(&quot;symbol&quot;, &quot;price&quot;, &quot;size&quot;)) The function returns an S3 object of class iex_api which has three accessible fields: path , response and content. The path contains the corresponding IEX API path: dat$path ## [1] &quot;tops/last&quot; The response contains the unparsed IEX API response: dat$response ## Response [https://api.iextrading.com/1.0/tops/last?symbols=AAPL&amp;filter=symbol%2Cprice%2Csize] ## Date: 2019-02-17 23:16 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 45 B The content contains the parsed content from the API’s response: dat$content ## [[1]] ## [[1]]$symbol ## [1] &quot;AAPL&quot; ## ## [[1]]$price ## [1] 142 ## ## [[1]]$size ## [1] 100 According to the developer, this package causes R to pause 0.2 seconds after executing an API call to avoid the user being throttled by the IEX API (which enforces a 5 request per second limit). Documentation about the other set of functions can be obtained at https://github.com/imanuelcostigan/iex/tree/master/man. 1.2.3 Quandl 1.2.4 SEC Official filings are freely available from the U.S. Securities and Exchange Commission’s EDGAR database. The package finreportr provides an interface in R to facilitate financial analysis from SEC’s 10K and 10K/A filings. We can obtain company basic information with the function the CompanyInfo function by passing the ticker symbol of the target company as follows: library(&quot;finreportr&quot;) AAPL.Info&lt;-CompanyInfo(&quot;AAPL&quot;) print(AAPL.Info) ## company CIK SIC state state.inc FY.end street.address ## 1 APPLE INC 0000320193 3571 CA CA 0930 ONE APPLE PARK WAY ## city.state ## 1 CUPERTINO CA 95014 As a result, we obtain the following information: Company name: APPLE INC; SEC Central Index Key (CIK): 0000320193; Standard Industrial Classification (SIC): 3571, which is the industry code for Electronic Computers; Address: ONE APPLE PARK WAY, CUPERTINO CA 95014; Most recent period of report end is 0930. The list of company annual reports with corresponding filing dates can be obtained with the function AnnualReports as follows: AAPL.reports&lt;-AnnualReports(&quot;AAPL&quot;) Table 1.1: Sample Annual Reports filing.name filing.date accession.no 10-K 2018-11-05 0000320193-18-000145 10-K 2017-11-03 0000320193-17-000070 10-K 2016-10-26 0001628280-16-020309 10-K 2015-10-28 0001193125-15-356351 10-K 2014-10-27 0001193125-14-383437 10-K 2013-10-30 0001193125-13-416534 The accession number is a unique identifier that the SEC creates for each filing. Company financials are organized into 3 segments: Income Statement, Balance Sheet and Cash Flow. Income Statement Financials from the Income Statement segment can be obtained with the GetIncome function as follows: AAPL.IS&lt;-GetIncome(&quot;AAPL&quot;, 2017) Table 1.2: Sample Income Statement Financials Metric Units Amount startDate endDate Revenue, Net usd 233715000000 2014-09-28 2015-09-26 Revenue, Net usd 75872000000 2015-09-27 2015-12-26 Revenue, Net usd 50557000000 2015-12-27 2016-03-26 Revenue, Net usd 42358000000 2016-03-27 2016-06-25 Revenue, Net usd 46852000000 2016-06-26 2016-09-24 Revenue, Net usd 215639000000 2015-09-27 2016-09-24 The Income Statement function returns data for the following metrics: Table 1.3: Income Statement Metrics Metrics Revenue, Net Cost of Goods and Services Sold Gross Profit Research and Development Expense Selling, General and Administrative Expense Operating Expenses Operating Income (Loss) Nonoperating Income (Expense) Income (Loss) from Continuing Operations before Income Taxes, Noncontrolling Interest Income Tax Expense (Benefit) Net Income (Loss) Attributable to Parent Earnings Per Share, Basic Earnings Per Share, Diluted Weighted Average Number of Shares Outstanding, Basic Weighted Average Number of Shares Outstanding, Diluted Common Stock, Dividends, Per Share, Declared Balance Sheet Financials from the Balance Sheet segment can be obtained with the GetBalanceSheet function as follows: AAPL.BS&lt;-GetBalanceSheet(&quot;AAPL&quot;, 2017) Table 1.4: Sample Balance Sheet Financials Metric Units Amount startDate endDate Cash and Cash Equivalents, at Carrying Value usd 13844000000 NA 2014-09-27 Cash and Cash Equivalents, at Carrying Value usd 21120000000 NA 2015-09-26 Cash and Cash Equivalents, at Carrying Value usd 20484000000 NA 2016-09-24 Cash and Cash Equivalents, at Carrying Value usd 20289000000 NA 2017-09-30 Available-for-sale Securities, Current usd 46671000000 NA 2016-09-24 Available-for-sale Securities, Current usd 53892000000 NA 2017-09-30 The Balance Sheet function returns data for the following metrics: Table 1.5: Balance Sheet Metrics Metrics Cash and Cash Equivalents, at Carrying Value Available-for-sale Securities, Current Accounts Receivable, Net, Current Inventory, Net Nontrade Receivables, Current Other Assets, Current Assets, Current Available-for-sale Securities, Noncurrent Property, Plant and Equipment, Net Goodwill Intangible Assets, Net (Excluding Goodwill) Other Assets, Noncurrent Assets Accounts Payable, Current Accrued Liabilities, Current Deferred Revenue, Current Commercial Paper Long-term Debt, Current Maturities Liabilities, Current Deferred Revenue, Noncurrent Long-term Debt, Excluding Current Maturities Other Liabilities, Noncurrent Liabilities Commitments and Contingencies Common Stocks, Including Additional Paid in Capital Retained Earnings (Accumulated Deficit) Accumulated Other Comprehensive Income (Loss), Net of Tax Stockholders’ Equity Attributable to Parent Liabilities and Equity Cash Flow Financials from the Cash Flow segment can be obtained with the GetCashFlow function as follows: AAPL.CF&lt;-GetCashFlow(&quot;AAPL&quot;, 2017) Table 1.6: Sample Cash Flow Financials Metric Units Amount startDate endDate Cash and Cash Equivalents, at Carrying Value usd 13844000000 NA 2014-09-27 Cash and Cash Equivalents, at Carrying Value usd 21120000000 NA 2015-09-26 Cash and Cash Equivalents, at Carrying Value usd 20484000000 NA 2016-09-24 Cash and Cash Equivalents, at Carrying Value usd 20289000000 NA 2017-09-30 Net Income (Loss) Attributable to Parent usd 53394000000 2014-09-28 2015-09-26 Net Income (Loss) Attributable to Parent usd 18361000000 2015-09-27 2015-12-26 The Cash Flow function returns data for the following metrics: Table 1.7: Cash Flow Metrics Metrics Cash and Cash Equivalents, at Carrying Value Net Income (Loss) Attributable to Parent Depreciation, Amortization and Accretion, Net Share-based Compensation Deferred Income Tax Expense (Benefit) Other Noncash Income (Expense) Increase (Decrease) in Accounts Receivable Increase (Decrease) in Inventories Increase (Decrease) in Other Receivables Increase (Decrease) in Other Operating Assets Increase (Decrease) in Accounts Payable Increase (Decrease) in Deferred Revenue Increase (Decrease) in Other Operating Liabilities Net Cash Provided by (Used in) Operating Activities Payments to Acquire Available-for-sale Securities Proceeds from Maturities, Prepayments and Calls of Available-for-sale Securities Proceeds from Sale of Available-for-sale Securities Payments to Acquire Businesses, Net of Cash Acquired Payments to Acquire Property, Plant, and Equipment Payments to Acquire Intangible Assets Payments to Acquire Other Investments Payments for (Proceeds from) Other Investing Activities Net Cash Provided by (Used in) Investing Activities Proceeds from Issuance of Common Stock Excess Tax Benefit from Share-based Compensation, Financing Activities Payments Related to Tax Withholding for Share-based Compensation Payments of Dividends Payments for Repurchase of Common Stock Proceeds from Issuance of Long-term Debt Repayments of Long-term Debt Proceeds from (Repayments of) Commercial Paper Net Cash Provided by (Used in) Financing Activities Cash and Cash Equivalents, Period Increase (Decrease) Income Taxes Paid, Net Interest Paid 1.3 Conclusion We showed how to load and import data from both local files and external sources. We provided examples on how to read tabular data and how to handle large files. We showed how to obtain financial and economic data from freely available sources. 1.3.1 Further Reading To further learn how to use R to load, transform, visualize and model data see (Wickham and Grolemund 2017). Additional relevant R packages include the following: dplyr: Fast data frames manipulation and database query. reshape2: Flexibly rearrange, reshape and aggregate data. readr: A fast and friendly way to read tabular data into R. tidyr: Easily tidy data with spread and gather functions. rlist: A toolbox for non-tabular data manipulation with lists. jsonlite: A robust and quick way to parse JSON files in R. ff: Data structures designed to store large datasets. lubridate: A set of functions to work with dates and times. References "],
["stylized-facts.html", "Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.3 Volatility 2.4 Correlation", " Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.2.1 Fat Tails 2.2.2 Skewness 2.3 Volatility 2.3.1 Time-invariance 2.3.2 Volatility Clustering 2.3.3 Correlation with Trading Volume 2.4 Correlation 2.4.1 Time-invariance 2.4.2 Auto-correlation "],
["limit-order.html", "Chapter 3 Limit Order", " Chapter 3 Limit Order "],
["entropy.html", "Chapter 4 Entropy 4.1 Definition 4.2 Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets", " Chapter 4 Entropy 4.1 Definition Let \\(X\\) be a random variable and \\(P_X(x)\\) be its probability density function (pdf). The entropy \\(H(X)\\) is a measure of the uncertainty of \\(X\\) and is defined in the discrete case as follows: \\[\\begin{equation} H(X) = -\\sum_{x \\in X}{P_X(x)\\log{P_X(x)}}. \\label{eq:H} \\end{equation}\\] If the \\(\\log\\) is taken to base two, then the unit of \\(H\\) is the (binary digit). We employ the natural logarithm which implies the unit in (natural unit of information). 4.2 Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets "],
["information-theory-and-statistical-causality.html", "Chapter 5 Information Theory and Statistical Causality 5.1 A First Definition of Causality 5.2 A Probabilistic-Based Definition 5.3 Transfer Entropy and Statistical Causality 5.4 Net Information Flow 5.5 The Link Between Granger-causality and Transfer Entropy 5.6 Empirical Experiment: Information Flow on Simulated Systems 5.7 Empirical Experiment: Information Flow on Global Markets", " Chapter 5 Information Theory and Statistical Causality Figure 5.1: Life is Random (or Nonlinear?) \\[\\begin{align*} x_1 &amp;= 0.441x_1 + \\epsilon_1 \\\\ x_2 &amp;= 0.51x_1^2 + \\epsilon_2, \\\\ &amp;\\epsilon_1, \\epsilon_2 \\sim \\mathcal{N}(0,1). \\end{align*}\\] We quantify causality by using the notion of the causal relation introduced by Granger (Wiener 1956; Granger 1969) where a signal \\(X\\) is said to Granger-cause \\(Y\\) if the future realizations of \\(Y\\) can be better explained using the past information from \\(X\\) and \\(Y\\) rather than \\(Y\\) alone. 5.1 A First Definition of Causality The most common definitions of Granger-causality rely on the prediction of a future value of the variable \\(Y\\) by using the past values of \\(X\\) and \\(Y\\) itself. In that form, \\(X\\) is said to G-cause \\(Y\\) if the use of \\(X\\) improves the prediction of \\(Y\\). Let \\(X_t\\) be a random variable associated at time \\(t\\) while \\(X^t\\) represents the collection of random variables up to time \\(t\\). We consider \\({X_t}, {Y_t}\\) and \\({Z_t}\\) to be three stochastic processes. Let \\(\\hat Y_{t+1}\\) be a predictor for the value of the variable \\(Y\\) at time \\(t+1\\). We compare the expected value of a loss function \\(g(e)\\) with the error \\(e=\\hat{Y}_{t+1} - Y_{t+1}\\) of two models: The expected value of the prediction error given only \\(Y^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_1(X^{t},Z^t))] \\end{equation}\\] The expected value of the prediction error given \\(Y^t\\) and \\(X^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, X^{t},Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_2(X^{t},Y^t,Z^t))]. \\end{equation}\\] In both models, the functions \\(f_1(.)\\) and \\(f_2(.)\\) are chosen to minimize the expected value of the loss function. In most cases, these functions are retrieved with linear and, possibly, with nonlinear regressions. Typical forms for \\(g(.)\\) are the \\(l1\\)- or \\(l2\\)-norms. We can now provide our first definition of statistical causality under the Granger causal notion as follows: Definition 5.1 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(\\mathcal{R}(Y_{t+1} \\; | \\; X^t, Y^t, Z^t) = \\mathcal{R}(Y_{t+1} \\; | \\; Y^t, Z^t)\\). Standard Granger-causality tests assume a functional form in the relationship among the causes and effects and are implemented by fitting autoregressive models (Wiener 1956; Granger 1969). Consider the linear vector-autoregressive (VAR) equations: \\[\\begin{align} Y(t) &amp;= {\\alpha} + \\sum^k_{\\Delta t=1}{{\\beta}_{\\Delta t} Y(t-\\Delta t)} + \\epsilon_t, \\tag{5.1}\\\\ Y(t) &amp;= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} Y(t-\\Delta t)} + \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}X(t-\\Delta t)}+ \\widehat{\\epsilon}_t, \\tag{5.2} \\end{align}\\] where \\(k\\) is the number of lags considered. Alternatively, you can choose your DL/SVM/RF/GLM method of choice to fit the model. From Def 5.1, \\(X\\) does not G-cause \\(Y\\) if and only if the prediction errors of \\(X\\) in the restricted Eq. (5.1) and unrestricted regression models Eq. (5.2) are equal (i.e., they are statistically indistinguishable). A one-way ANOVA test can be utilized to test if the residuals from Eqs. (5.1) and (5.2) differ from each other significantly. When more than one lag \\(k\\) is tested, a Bonferroni correction is applied to control for multiple hypotheses testing. 5.2 A Probabilistic-Based Definition A more general definition than Def. 5.1 that does not depend on assuming prediction functions can be formulated by considering conditional probabilities. A probabilistic definition of G-causality assumes that \\(Y_{t+1}\\) and \\(X^{t}\\) are independent given the past information \\((X^{t}, Y^{t})\\) if and only if \\(p(Y_{t+1} \\, | \\, X^{t}, Y^{t}, Z^{t}) = p(Y_{t+1} \\, | \\, Y^{t}, Z^{t})\\), where \\(p(. \\, | \\, .)\\) represents the conditional probability distribution. In other words, omitting past information from \\(X\\) does not change the probability distribution of \\(Y\\). This leads to our second definition of statistical causality as follows: Definition 5.2 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(Y_{t+1} {\\perp\\!\\!\\!\\!\\perp}X^{t} \\; | \\; Y^{t}, Z^{t}\\). Def. 5.2 does not assume any functional form in the coupling between \\(X\\) and \\(Y\\). Nevertheless, it requires a method to assess their conditional dependency. In the next section, we will leverage an Information-Theoretical framework for that purpose. 5.3 Transfer Entropy and Statistical Causality Given a coupled system \\((X,Y)\\), where \\(P_Y(y)\\) is the pdf of the random variable \\(Y\\) and \\(P_{X,Y}\\) is the joint pdf between \\(X\\) and \\(Y\\), the joint entropy between \\(X\\) and \\(Y\\) is given by the following: \\[\\begin{equation} H(X,Y) = -\\sum_{x \\in X}{\\sum_{y \\in Y}{P_{X,Y}(x,y)\\log{P_{X,Y}(x,y)}}}. \\label{eq:HXY} \\end{equation}\\] The conditional entropy is defined by the following: \\[\\begin{equation} H\\left(Y\\middle\\vert X\\right) = H(X,Y) - H(X). \\end{equation}\\] We can interpret \\(H\\left(Y\\middle\\vert X\\right)\\) as the uncertainty of \\(Y\\) given a realization of \\(X\\). To compute G-Causality, we use the concept of Transfer Entropy. Since its introduction (Schreiber 2000), Transfer Entropy has been recognized as an important tool in the analysis of causal relationships in nonlinear systems (Hlavackovaschindler et al. 2007). It detects directional and dynamical information (Montalto 2014) while not assuming any particular functional form to describe interactions among systems. The Transfer Entropy can be defined as the difference between the conditional entropies: \\[\\begin{equation} TE\\left(X \\rightarrow Y\\right \\vert Z) = H\\left(Y^F\\middle\\vert Y^P,Z^P\\right) - H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right), \\tag{5.3} \\end{equation}\\] which can be rewritten as a sum of Shannon entropies: \\[\\begin{align} TE\\left(X \\rightarrow Y\\right) = H\\left(Y^P, X^P\\right) - H\\left(Y^F, Y^P, X^P\\right) + H\\left(Y^F, Y^P\\right) - H\\left(Y^P\\right), \\end{align}\\] where \\(Y^F\\) is a forward time-shifted version of \\(Y\\) at lag \\(\\Delta t\\) relatively to the past time-series \\(X^P\\), \\(Y^P\\) and \\(Z^P\\). Within this framework we say that \\(X\\) does not G-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(H\\left(Y^F\\middle\\vert Y^P,Z^P \\right) = H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right)\\), i.e., when \\(TE\\left(X \\rightarrow Y,Z^P\\right) = 0\\). 5.4 Net Information Flow Transfer-entropy is an asymmetric measure, i.e., \\(T_{X \\rightarrow Y} \\neq T_{Y \\rightarrow X}\\), and it thus allows the quantification of the directional coupling between systems. The Net Information Flow is defined as \\[\\begin{equation} \\widehat{TE}_{X \\rightarrow Y} = TE_{X \\rightarrow Y} - TE_{Y \\rightarrow X}\\;. \\end{equation}\\] One can interpret this quantity as a measure of the dominant direction of the information flow. In other words, a positive result indicates a dominant information flow from \\(X\\) to \\(Y\\) compared to the other direction or, similarly, it indicates which system provides more predictive information about the other system (Michalowicz, Nichols, and Bucholtz 2013). 5.5 The Link Between Granger-causality and Transfer Entropy It has been shown (Barnett, Barrett, and Seth 2009) that linear G-causality and Transfer Entropy are equivalent if all processes are jointly Gaussian. In particular, by assuming the standard measure (\\(l2\\)-norm loss function) of linear G-causality for the bivariate case as follows (see Section 5.1 for more details on linear-Granger causality): \\[\\begin{equation} GC_{X \\rightarrow Y} = \\log\\left( \\frac{var(\\epsilon_t)}{var( \\widehat{\\epsilon}_t)} \\right), \\tag{5.4} \\end{equation}\\] the following can be proved (Barnett, Barrett, and Seth 2009): \\[\\begin{align} TE_{X \\rightarrow Y} = GC_{X \\rightarrow Y}/2. \\tag{5.5} \\end{align}\\] This result provides a direct mapping between the Transfer Entropy and the linear G-causality implemented in the standard VAR framework. Hence, it is possible to estimate the TE both in its general form and with its equivalent form for linear G-causality. 5.6 Empirical Experiment: Information Flow on Simulated Systems In this section, we construct simulated systems to couple random variables in a causal manner. We then quantify information flow using the methods studied in this Chapter. We first assume a linear system, where random variables have linear relationships defined as follow: \\[\\begin{align} x_1(n) &amp;= 0.95\\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\\\ \\nonumber x_2(n) &amp;= 0.5x_1(n-1) + w_2\\\\ \\nonumber x_3(n) &amp;= -0.4x_1(n-1) + w_3\\\\ \\nonumber x_4(n) &amp;= -0.5x_1(n-1) + 0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_4\\\\ \\nonumber x_5(n) &amp;= -0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_5, \\nonumber \\end{align}\\] \\(w_1, w_2, w_3, w_4, w_5 \\sim N(0, 1)\\). Fig. 5.2 provides a representation of the causal links in the simulated system. Figure 5.2: Interactions between the variables of the simulated linear system. Nonlinear Systems: \\[\\begin{align} x_1(n) &amp;= 0.95\\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\\\ \\nonumber x_2(n) &amp;= \\textcolor{red}{0.5x_1^2(n-1)} + w_2\\\\ \\nonumber x_3(n) &amp;= -0.4x_1(n-1) + w_3\\\\ \\nonumber x_4(n) &amp;= \\textcolor{red}{-0.5x_1^2(n-1)} + 0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_4\\\\ \\nonumber x_5(n) &amp;= -0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_5, \\nonumber \\end{align}\\] \\(w_1, w_2, w_3, w_4, w_5 \\sim N(0, 1)\\). Figure 5.3: Interactions between the variables of the simulated nonlinear system. 5.7 Empirical Experiment: Information Flow on Global Markets References "],
["financial-networks.html", "Chapter 6 Financial Networks 6.1 Introduction 6.2 Network Construction 6.3 Applications", " Chapter 6 Financial Networks 6.1 Introduction Financial markets can be regarded as a complex network in which nodes represent different financial assets and edges represent one or many types of relationships among those assets. Filtered correlation-based networks have successfully been used in the literature to study financial markets structure particularly from observational data derived from empirical financial time series (Bardoscia et al. 2017; S. A. L. Tumminello Michele AND Miccichè 2011; R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010, M. Tumminello et al. (2005)). The underlying principle is to use correlations from empirical financial time series to construct a sparse network representing the most relevant connections. Analyses on filtered correlation-based networks for information extraction (Song, Aste, and Di Matteo 2008; T. Aste, Shaw, and Di Matteo 2010) have widely been used to explain market interconnectedness from high-dimensional data. Applications include asset allocation (Y. Li et al. 2018; Pozzi, Di Matteo, and Aste 2013), market stability assessments (Morales et al. 2012), hierarchical structure analyses (R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010; Musmeci, Aste, and Matteo 2014; Song, Di Matteo, and Aste 2012) and the identification of lead-lag relationships (Curme, Stanley, and Vodenska 2015). In this Chapter we will describe how to Construct and filter financial networks; Build price-based dynamic industry taxonomies; Implement a trading strategy based on financial network structure. 6.2 Network Construction We selected \\(N = 100\\) of the most capitalized companies that were part of the S&amp;P500 index from 09/05/2012 to 08/25/2017. The list of these companies’ ticker symbols is reported in the Appendix . For each stock \\(i\\) the financial variable was defined as the daily stock’s log-return \\(R_i(\\tau)\\) at time \\(\\tau\\). Stock returns \\(R_i\\) and social media opinion scores \\(O_i\\) each amounted to a time series of length equals to 1251 trading days. These series were divided time-wise into \\(M = 225\\) windows \\(t = 1, 2, \\ldots, M\\) of width \\(T = 126\\) trading days. A window step length parameter of \\(\\delta T = 5\\) trading days defined the displacement of the window, i.e., the number of trading days between two consecutive windows. The choice of window width \\(T\\) and window step \\(\\delta T\\) is arbitrary, and it is a trade-off between having analysis that is either too dynamic or too smooth. The smaller the window width and the larger the window steps, the more dynamic the data are. To characterize the synchronous time evolution of assets, we used equal time Kendall’s rank coefficients between assets \\(i\\) and \\(j\\), defined as \\[\\begin{equation} \\rho_{i, j}(t) = \\sum\\limits_{t&#39; &lt; \\tau}sgn(V_i(t&#39;) - V_i(\\tau))sgn(V_j(t&#39;) - V_j(\\tau)), \\end{equation}\\] where \\(t&#39;\\) and \\(\\tau\\) are time indexes within the window \\(t\\) and \\(V_i \\in \\{R_i, O_i\\}\\). Kendall’s rank coefficients takes into account possible nonlinear (monotonic) relationships. It fulfill the condition \\(-1 \\leq \\rho_{i, j} \\leq 1\\) and form the \\(N \\times N\\) correlation matrix \\(C(t)\\) that served as the basis for the networks constructed in this work. To construct the asset-based financial and social networks, we defined a distance between a pair of stocks. This distance was associated with the edge connecting the stocks, and it reflected the level at which they were correlated. We used a simple non-linear transformation \\(d_{i, j}(t) = \\sqrt{2(1 - \\rho_{i,j}(t))}\\) to obtain distances with the property \\(2 \\geq d_{i,j} \\geq 0\\), forming a \\(N \\times N\\) symmetric distance matrix \\(D(t)\\). 6.2.1 Network Filtering: Asset Graphs We extract the \\(N(N-1)/2\\) distinct distance elements from the upper triangular part of the distance matrix \\(D(t)\\), which were then sorted in an ascending order to form an ordered sequence \\(d_1(t), d_2(t), \\ldots, d_{N(N-1)/2}(t)\\). Since we require the graph to be representative of the market, it is natural to build the network by including only the strongest connections. This is a network filtering procedure that has been successfully applied in the construction of for the analyses of market structure . The number of edges to include is arbitrary, and we included those from the bottom quartile, which represented the 25% shortest edges in the graph (largest correlations), thus giving \\(E(t) = \\{d_1(t), d_2(t), \\ldots, d_{\\floor{N/4}}(t)\\}\\). We denoted \\(E^{F}(t)\\) as the set of edges constructed from the distance matrix derived from stock returns \\(R(t)\\). The financial network considered is \\(G^{F} = ( V, E^{F} )\\), where \\(V\\) is the vertex set of stocks. 6.2.2 Network Filtering: MST 6.2.3 Network Filtering: PMFG 6.3 Applications 6.3.1 Industry Taxonomy 6.3.2 Portfolio Construction References "],
["the-market-the-players-and-the-rules.html", "Chapter 7 The Market, The Players and The Rules 7.1 The Market 7.2 The Data 7.3 The Buyers 7.4 Conclusion", " Chapter 7 The Market, The Players and The Rules 7.1 The Market In an industry where almost every player has access to the same (or similar) core financial and fundamental data, investment firms look for Alternative Data seeking differentiated insights into companies not found in filings, earnings calls or fundamental datasets. Until recently, the usage of alternative data has been confined mainly to the realm of quantitative investment managers, as these firms were best suited to obtain, clean and process this data. Now, however, alternative data is beginning to go mainstream, with increasing interest from fundamental and hybrid asset managers here (Johnson 2019). Alternative Data is Untapped Alpha. The biggest opportunity for investors in this decade comes from the signals buried in the data generated by the digital economy. Alternative data is the deepest, least utilized alpha source in the world today - Quandl. On June/2019, alternative data went mainstream when Verizon made a bold move by launching a subscription service for Yahoo Finance that offers alternative data and insights to retail investors at a price point of $34.99/month. The amount spent in Alternative Data have been steadily increasing. More than half of all quantitative and fundamental investors surveyed recently are either considering or using alternative data as part of their workflow - Greenwich Associates. A recent Greenwich Associates report found that a wide majority of investment managers, 72%, stated that alternative data was improving enhanced their signal quality in an arena where filtering out signal noise. Of those who are implementing an alternative data strategy, more than one-fifth claim to have received 20% or more of their alpha from the practice. Figure 7.1: Total Buy-side spend on Alternative Data has been steadily increasing and its likely to nearly triple from 2018 to 2020. 7.2 The Data But where’s Alternative Data coming from and who are the key players providing access to it? Figure 7.2: Alternative Data are sourced by heterogeneous sources including individuals, businesses and sensors. The growth of Alternative Data has been enabled by the digitalization of the world around us. Data have been produced at an unprecedented rate by heterogeneous sources including: Individuals who today mirror their lives in the form digital behavior into the Web, Social Media and Apps; Sensors, particularly with the emergence of IoT, that generate satellite images, weather forecasts and geolocation data; Businesses from which data is generated in the form of traditional official filings and earning calls as well as banking records or supply chain data. Figure 7.3: The number of Alternative Data Providers is exploding. The number of Alternative Data Providers is exploding.The number of alternative data vendors is exploding particularly in the past decade. A few companies are worth mentioning: Quandl is likely the largest financial and alternative data aggregator/provider today. They leverage relationships with third-party providers to be a one-stop-shop for alternative data from consumers, IoT and sentiment to traditional fundamental, pricing and estimates. Quandl has been recently acquired by Nasdaq. Dataminr leverages advanced anomaly detection technology in a real-time AI platform to uncover critical events from social media before the information goes mainstream. The company raised a staggering $500MM+ amount in funding so far and it gives no sign of that is slowing down. Thinknum provides a SaaS-based web platform for a variety of alternative data sets, in particular, web-scrapped data such as product comparisons and company ratings on jobs sites. Thinkum claims to have 8 of the 10 largest investment banks as their clients. They recently closed a $11.6 million Series A funding to expand their financial modeling tools. Yewno is helping the world to uncover the undiscovered through its advanced dynamic Knowledge Graph and AI based inference engine, which introduces an entirely new approach to knowledge extraction to enhance human understanding by correlating concepts across a vast volume of sources. Yewno ingests data from sources such as clinical trials, patents, company transcripts and court opinions and more. Yewno raised about $30 million dollars in funding and it has launched AI-based equity indexes with major partners such as STOXX and Nasdaq and licensed alternative data feeds to buy-side firms. Other rising vendors include YipitData, 1010data and Enigma as well as incumbent players such as Refinitiv, S&amp;P Global Market Intelligence, Bloomberg and Factset. While the alternative data offering is exploding there is a gap between what data vendors offer and what data buyers want. A research report conducted by BattleFin and AlternativeData.org has found that there is a gap between what data vendors offer and what data buyers actually want: - Data buyers view the three most-valuable data categories as: credit/debit cards (14.83%) web data (12.29%), and social/sentiment (11.02%). - By contrast, the percentage of data providers covering these data categories are: credit card (0.69%), web-crawled data (3.94%) and sentiment (4.40%). - Data providers report that the top-three categories their data covers are: business insights (6.48%), web traffic (4.63%) and sentiment (4.40%). Figure 7.4: Data extracted from a research report conducted by BattleFin and AlternativeData.org, which compiles responses from 173 respondents made up of 69 alternative-data buyers and 104 data providers who responded to a 2018 survey conducted by BattleFin. 7.3 The Buyers The data buyers include your usual suspects such as Two Sigma, Milennium, Third Point, WorldQuant, ExodusPoint and emerging Quant Funds like Credit Suisse’s QT Fund. 80% of buyers expect to purchase a minimum of one to five datasets in 2019 while nearly 60 percent expects to test and evaluate a minimum of one to five datasets this year - according to a research report conducted by BattleFin and AlternativeData.org. Pension Plans and Index Providers are also entering the space seeking exposure to alternative factors, particularly with the emergence of AI-based strategies that leverage machine learning and computational linguistics techniques to extract signals from unstructured data (see STOXX’s or DWS’s AI-based indexes) as well as the increased interest in themes such as ESG (Environmental, Social and Corporate Governance) - a multi-dimensional multi-sourced theme with increasing demands that are no longer met by traditional data sources alone. I have been to a good number of meetings with data buyers (BattleFin is maybe the best event out there for 1-on-1 meetings with high-profile buy-side firms sometimes allowing for 30+ meetings in two days of events in addition to dozens of unofficial ones). Some questions are commonplace: Can you describe your data sources and your data ingestion process? What’s the frequency/history/coverage? What’s the delivery method? What are the key use cases for your data? Can I run a trial with live data? Who are your clients? What’s pricing like? Do you offer exclusivity? What is the size/funding/team profile of your firm? There is one tricky question though: Whether the data was already licensed to other clients. Particularly if the buyer is looking for alpha, they actually would prefer to be one of the first to test the data and avoid using a dataset that is already “crowded”. By the same token, they would be happy to hear that the dataset was already licensed to reputable clients as a sign of credibility. It’s up to you to navigate through this trade-off. Figure 7.5: The sales life-cycle of an alternative data product can take up to seven months from lead generation to customer sign-up. Image by Quandl. This is just the very first step in the sales and marketing life-cycle. From lead generation to customer sign-up, data buyers can take several months to make a final decision. Here are some findings: Providers and buyers agree that alt-data sales cycle can take up to seven months, with more than 75% of buyers requiring up to six months to test, evaluate and purchase an alt dataset. More than a third of data buyers use backtesting to inform their purchasing decision. Nearly 60% of providers are not interested in outsourcing the data testing process even if it can be handled efficiently. More than 20% of providers sell and distribute using a customized alt-data software platform. Nearly 18% use no software platform at all. 7.4 Conclusion Alternative Data, once a treat only for the most advanced Hedge Funds, is becoming a must-have in the alpha-generation war on Wall Street. Total spending in the area has been steadily growing as well as the number of data vendors and variety of data products. Relevant acquisitions have been made in the space and the first unicorn startups have been formed. However, there are critical challenges in the industry. There is a gap between what buyers find most relevant and what the data vendors are producing. Further, the sales life-cycle is long, as players are still figuring out how to test the value of the data and considerable time is spent on on-boarding new datasets. Successful players will be those who bridge the gap between data and insights first. The race has just begun, and it looks like no one wants to be just a spectator. References "],
["statistical-methods.html", "A Statistical Methods A.1 Kernel Density Estimation", " A Statistical Methods This Appendix provides details to some of statistical methods used in the book. A.1 Kernel Density Estimation In the entropy computation (see Section 4) the empirical probability distribution must be estimated. Histogram-based methods and kernel density estimations are the two main methods for that. Histogram-based is the simplest and most used nonparametric density estimator. Nonetheless, it yields density estimates that have discontinuities and vary significantly depending on the bin size choice. Also known as the Parzen-Rosenblatt window method, the kernel density estimation (KDE) approach approximates the density function at point \\(x\\) using neighboring observations. However, instead of building up the estimate according to the bin edges as in histograms, the KDE method uses each point of estimation \\(x\\) as the center of a bin of width \\(2h\\) and weight it according to a kernel function. Thereby, the kernel estimate of the probability density function \\(f(x)\\) is defined as \\[\\begin{equation} \\hat{f} = \\frac{1}{nh}\\sum_{x&#39; \\in X}{K\\left(\\frac{x - x&#39;}{h}\\right)}. \\tag{A.1} \\end{equation}\\] A usual choice for the kernel \\(K\\), which we use here, is the (Gaussian) radial basis function: \\[\\begin{equation} K(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}x^2}. \\end{equation}\\] The problem of selecting the bandwidth \\(h\\) in Eq. (A.1) is crucial in the density estimation. A large \\(h\\) will oversmooth the estimated density and mask the structure of the data. On the other hand, a small bandwidth will reduce the bias of the density estimate at the expense of a larger variance in the estimates. If we assume that the true distribution is Gaussian and we use a Gaussian kernel, the optimal value of \\(h\\) that minimizes the mean integrated squared error (MISE) is \\[\\begin{equation*} h^* = 1.06\\sigma N^{-1/5}, \\end{equation*}\\] where \\(N\\) is the total number of points and \\(\\sigma\\) can be estimated as the sample standard deviation. This bandwidth estimation is often called the Gaussian approximation or Silverman’s rule of thumb for kernel density estimation (Silverman and Green 1986). This is the most commonly-used method and it is here employed. Other common methods are given by (Sheather and Jones 1991) and (Scott 1992). References "],
["references.html", "References", " References "]
]
