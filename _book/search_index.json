[
["index.html", "The Open Quant Live Book Preface", " The Open Quant Live Book OpenQuants.com 2019-08-26 Preface Description The book aims to be an Open Source introductory reference of the most important aspects of financial data analysis, algo trading, portfolio selection, econophysics and machine learning in finance with an emphasis in reproducibility and openness not to be found in most other typical Wall Street-like references. Contribute The Book is Open and we welcome co-authors. Feel free to reach out or simply create a pull request with your contribution! See project structure, guidelines and how to contribute here. Working Contents The Basics I/O Stylized Facts Algo Trading Investment Process Backtesting Factor Investing Limit Order Portfolio Optimization Convex Optimization Risk Parity Portfolios Machine Learning Intro Agent-Based Models Binary Classifiers AutoML Hierarchical Risk Parity Econophysics Entropy, Efficiency and Bubbles Nonparametric Statistical Causality: An Information-Theoretical Approach Financial Networks Alternative Data The Market, The Players, The Rules Case Studies Book’s information First published at: openquants.com. Licensed under Attribution-NonCommercial-ShareAlike 4.0 International. Copyright (c) 2019. OpenQuants.com, New York, NY. "],
["io.html", "Chapter 1 I/O 1.1 Importing Data 1.2 Data Sources 1.3 Conclusion", " Chapter 1 I/O In this Chapter, we will introduce basic functions to read text, excel and JSON files as well as large files. We will also show how to obtain free financial and economic data including the following: End-of-day and real-time pricing; Company financials; Macroeconomic data. Data sources utilized in this Chapter include the following: U.S. Securities and Exchange Commission; Quandl; IEX; Alpha Vantage. 1.1 Importing Data 1.1.1 Text Files The most basic and commonly used option to import data from text files in R is the use of the function read.table from the r-base. We can use this function to read text files with extensions such as .txt and .csv. dat.table &lt;- read.table(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- read.csv(file = &quot;&lt;name of your file&gt;.csv&quot;) The package readr provides functions for reading text data into R that are much faster that the functions from the r-base. The read_table function from the package readr provides a near-replacement for the read.table function. library(readr) dat.table &lt;- readr::read_table2(file = &quot;&lt;name of your file&gt;.txt&quot;) dat.csv &lt;- readr::read_csv(file = &quot;&lt;name of your file&gt;.csv&quot;) Another option to save data is to write it in rds format. Data stored in rds format has the advantage to keep the original data struture and type of the object saved. Also, .rds files are compressed and consume less space than files saved in .csv format. A data.frame object can be saved in rds format and then loaded back as follows: write_rds(dat.frame, path = &quot;&lt;name of your file&gt;.rds&quot;) dat.frame &lt;- read_rds(path = &quot;&lt;name of your file&gt;.rds&quot;) 1.1.2 Excel Files The package readxl has an ease to use interface to functions that load excel documents in R. The functions read_xls and read_xlsx can be used to read excel files as follows: library(readxl) readxl::read_xls(path = &quot;&lt;name of your file&gt;.xls&quot;) readxl::read_xlsx(path = &quot;&lt;name of your file&gt;.xlsx&quot;) The function read_excel() automatically detects the extension of the input file as follows: readxl::read_excel(&quot;&lt;name and extension of your file&gt;&quot;, sheet = &quot;&lt;sheet name or index&gt;&quot;) In the read_excel function, the sheet argument can receive either the target sheet name or index number, where sheet indexing starts at 1. The readxl has been oberving increased use compared to other comparable packages such as gdata and the xlsx due to its relative ease of use and performance. Also, the readxl do not have depency with external code libraries while the packages gdata and xlsx depend on ActiveState PERL and the Java JDK, respectively. 1.1.3 JSON Files JSON files are particularly used for transmitting data in web applications but also frequently used as a standard data interchange format. The jsonline package can be used to parse files in JSON format as follows: library(jsonlite) result_json &lt;- read_json(&quot;&lt;json file&gt;&quot;) 1.1.4 Large Files Fast data manipulation in a short and flexible syntax. 1.2 Data Sources In this section, we will show how to obtain financial and economic data from public sources. 1.2.1 Alpha Vantage Alpha Vantage offers free access to pricing data including: Stock Time Series Data; Physical and Digital/Crypto Currencies (e.g., Bitcoin); Technical Indicators and Sector Performances. The data are available in JSON and CSV formats via REST APIs. The quantmod and the alphavantager R packages offer a lightweight R interface to the Alpha Vantage API. Daily stock prices can be obtained with the quantmod::getSymbols function as follows: getSymbols(Symbols=&#39;AAPL&#39;, src=&quot;av&quot;, output.size=&quot;full&quot;, adjusted=TRUE, api.key=&#39;your API key&#39;) The output data is stored in an object with the same name as the corresponding symbol, in this example AAPL. The output data looks like the following AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume AAPL.Adjusted 60.4 60.8 59.9 60.4 1.26e+08 1.88 60.7 61.5 60.1 61.4 7.38e+07 1.91 61.1 63.1 61.1 62.1 1.01e+08 1.94 62.8 65.0 62.7 64.8 1.12e+08 2.02 65.0 65.0 62.0 62.1 8.41e+07 1.93 62.6 65.9 62.1 65.2 1.59e+08 2.03 We called the quantmod::getSymbols function with the following arguments: Symbols='AAPL' defines a character vector specifying the names of each symbol to be loaded, here specified by the symbol of the company Apple Inc.; src=&quot;av&quot; specifies the sourcing method, here defined with the value corresponding to Alpha Vantage; output.size=&quot;full&quot;specified length of the time series returned. The strings compact and full are accepted with the following specifications: compact returns only the latest 100 data points; full returns the full-length time series of up to 20 years of historical data; adjusted=TRUE defines a boolean variable to include a column of closing prices adjusted for dividends and splits; api.key specifies your Alpha Vantage API key. 1.2.2 IEX The IEX Group operates the Investors Exchange (IEX), a stock exchange for U.S. equities that is built for investors and companies. IEX offers U.S. reference and market data including end-of-day and intraday pricing data. IEX offers an API with “a set of services designed for developers and engineers. It can be used to build high-quality apps and services”. Data sourced from the IEX API is freely available for commercial subject to conditions and the use of their API is subject to additional terms of use. IEX lists the following github project as an unofficial API for R: https://github.com/imanuelcostigan/iex. We will provide examples on how to obtain intraday pricing data using this package. First, we will use the devtools to install the package directly from its github repository as follows: library(devtools) install_github(&quot;imanuelcostigan/iex&quot;) The iex package provides 4 set of functions as follows: last: Provides IEX near real time last sale price, size and time. Last is ideal for developers that need a lightweight stock quote. IEX API real time API documentation. market: Provides exchange trade volume data in near real time. IEX market API documentation. stats: A set of functions that return trading statistics. IEX stats API documentation. tops: Provides IEX’s aggregated bid and offer position in near real time for all securities on IEX’s displayed limit order book. IEX API TOPS documentation. For instance, the last function has the following arguments: symbols: A vector of tickers (case insensitive). Special characters will be escaped. A list of eligible symbols is published daily by the IEX. When set to NULL (default) returns values for all symbols. fields: A vector of fields names to return (case sensitive). When set to NULL (default) returns values for all fields. version: The API version number, which is used to define the API URL. We can obtain intraday stock price data with the last function as follows: dat &lt;- iex::last(symbols = c(&quot;AAPL&quot;), fields = c(&quot;symbol&quot;, &quot;price&quot;, &quot;size&quot;)) The function returns an S3 object of class iex_api which has three accessible fields: path , response and content. The path contains the corresponding IEX API path: dat$path ## [1] &quot;tops/last&quot; The response contains the unparsed IEX API response: dat$response ## Response [https://api.iextrading.com/1.0/tops/last?symbols=AAPL&amp;filter=symbol%2Cprice%2Csize] ## Date: 2019-08-26 05:32 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 46 B The content contains the parsed content from the API’s response: dat$content ## [[1]] ## [[1]]$symbol ## [1] &quot;AAPL&quot; ## ## [[1]]$price ## [1] 203 ## ## [[1]]$size ## [1] 300 According to the developer, this package causes R to pause 0.2 seconds after executing an API call to avoid the user being throttled by the IEX API (which enforces a 5 request per second limit). Documentation about the other set of functions can be obtained at https://github.com/imanuelcostigan/iex/tree/master/man. 1.2.3 Quandl Quandl is likely the largest financial and alternative data aggregator/provider today. They leverage relationships with third-party providers to be a one-stop-shop for alternative data and traditional fundamental, pricing and estimates datasets. Quandl offer an API which usage is free for registered users. You can obtain an API key here. After signing up, just append your API key to your call like this: https://www.quandl.com/api/v3/datasets/WIKI/FB/data.csv?api_key=YOURAPIKEYHERE At Quandl, every dataset is identified by “Quandl code”, which is a unique id. In the above example, you downloaded a dataset with the Quandl code “WIKI/FB”. Every Quandl code has 2 parts: the database code (“WIKI”) which specifies where the data comes from, and the dataset code (“FB”) which identifies the specific time series you want. You can find Quandl codes using their data browser. Additional API documentation can be found here. Quandl is also available via an R interface (Raymond McTaggart, Gergely Daroczi, and Clement Leung 2019). For instance, we can obtain Crude Oil Futures prices from 01/01/2010 to 01/01/2019 as follows: library(Quandl) Quandl.api_key(config::get()$quandl.key) from.dat &lt;- as.Date(&quot;01/01/2010&quot;, format=&quot;%d/%m/%Y&quot;) to.dat &lt;- as.Date(&quot;01/01/2019&quot;, format=&quot;%d/%m/%Y&quot;) crude.oil.futures&lt;-Quandl(&quot;CHRIS/CME_CL1&quot;, start_date = from.dat, end_date = to.dat, type=&quot;xts&quot;) plot(crude.oil.futures$Last) In the example above we specified the following Database/Dataset: Database: “CHRIS”. Continuous contracts for all 600 futures on Quandl. Built on top of raw data from CME, ICE, LIFFE etc. Curated by the Quandl community. 50 years history. Dataset: “CME_CL1”. Historical futures prices of Crude Oil Futures, Continuous Contract #1. Non-adjusted price based on spot-month continuous contract calculations. Raw data from CME. 1.3 Conclusion We showed how to load and import data from both local files and external sources. We provided examples on how to read tabular data and how to handle large files. We showed how to obtain financial and economic data from freely available sources. 1.3.1 Further Reading To further learn how to use R to load, transform, visualize and model data see (Wickham and Grolemund 2017). Additional relevant R packages include the following: dplyr: Fast data frames manipulation and database query. reshape2: Flexibly rearrange, reshape and aggregate data. readr: A fast and friendly way to read tabular data into R. tidyr: Easily tidy data with spread and gather functions. rlist: A toolbox for non-tabular data manipulation with lists. jsonlite: A robust and quick way to parse JSON files in R. ff: Data structures designed to store large datasets. lubridate: A set of functions to work with dates and times. References "],
["stylized-facts.html", "Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.3 Volatility 2.4 Correlation", " Chapter 2 Stylized Facts 2.1 Introduction 2.2 Distribution of Returns 2.2.1 Fat Tails 2.2.2 Skewness 2.3 Volatility 2.3.1 Time-invariance 2.3.2 Volatility Clustering 2.3.3 Correlation with Trading Volume 2.4 Correlation 2.4.1 Time-invariance 2.4.2 Auto-correlation "],
["entropy.html", "Chapter 3 Entropy 3.1 Definition 3.2 Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets", " Chapter 3 Entropy 3.1 Definition Let \\(X\\) be a random variable and \\(P_X(x)\\) be its probability density function (pdf). The entropy \\(H(X)\\) is a measure of the uncertainty of \\(X\\) and is defined in the discrete case as follows: \\[\\begin{equation} H(X) = -\\sum_{x \\in X}{P_X(x)\\log{P_X(x)}}. \\label{eq:H} \\end{equation}\\] If the \\(\\log\\) is taken to base two, then the unit of \\(H\\) is the (binary digit). We employ the natural logarithm which implies the unit in (natural unit of information). 3.2 Efficiency and Bubbles: A Case Study in the Crypto and Equity Markets "],
["how-to-measure-statistical-causality-a-transfer-entropy-approach-with-financial-applications.html", "Chapter 4 How to Measure Statistical Causality: A Transfer Entropy Approach with Financial Applications 4.1 A First Definition of Causality 4.2 A Probabilistic-Based Definition 4.3 Transfer Entropy and Statistical Causality 4.4 Net Information Flow 4.5 The Link Between Granger-causality and Transfer Entropy 4.6 Information Flow on Simulated Systems 4.7 Information Flow among International Stock Market Indices 4.8 Other Applications 4.9 Conclusions", " Chapter 4 How to Measure Statistical Causality: A Transfer Entropy Approach with Financial Applications We’ve all heard the say “correlation does not imply causation”, but how can we quantify causation? This is an extremely difficult and often misleading task, particularly when trying to infer causality from observational data and we cannot perform controlled trials or A/B testing. Take for example the 2-dimensional system from Fig. 4.1. Figure 4.1: Life is Random (or Nonlinear?) At a first glance, one could say that there is no clear relationship or causality between the random variables \\(x_1\\) and \\(x_2\\). However, this apparent random system presents a causal relationship defined by the following simple equations: \\[\\begin{align*} x_1(n) &amp;= 0.441x_1(n-1) + \\epsilon_1 \\\\ x_2(n) &amp;= 0.51x_1^2(n-1) + \\epsilon_2, \\\\ &amp;\\epsilon_1, \\epsilon_2 \\sim \\mathcal{N}(0,1). \\end{align*}\\] A simple nonlinearity introduced in the relationship between \\(x_2\\) and \\(x_1\\) was enough to introduce complexity into the system and potentially mislead a naive (non-quant) human. Fortunately, we can take advantage of statistics and information theory to uncover complex causal relationships from observational data (remember, this is still a very challenging task). The objectives of this Chapter are the following: Introduce a prediction-based definition of causality and its implementation using a vector auto-regression formulation. Introduce a probabilistic-definition of causality and its implementation using an information-theoretical framework. Simulate linear and nonlinear systems and uncover causal links with the proposed methods. Quantify information flow among global equity indexes further uncovering which indexes are driving the global financial markets. Discuss further applications including the impact of social media sentiment in financial and crypto markets. 4.1 A First Definition of Causality We quantify causality by using the notion of the causal relation introduced by Granger (Wiener 1956; Granger 1969), where a signal \\(X\\) is said to Granger-cause \\(Y\\) if the future realizations of \\(Y\\) can be better explained using the past information from \\(X\\) and \\(Y\\) rather than \\(Y\\) alone. Figure 4.2: Economist Clive Granger, who won the 2003 Nobel Prize in Economics. The most common definitions of Granger-causality (G-causality) rely on the prediction of a future value of the variable \\(Y\\) by using the past values of \\(X\\) and \\(Y\\) itself. In that form, \\(X\\) is said to G-cause \\(Y\\) if the use of \\(X\\) improves the prediction of \\(Y\\). Let \\(X_t\\) be a random variable associated at time \\(t\\) while \\(X^t\\) represents the collection of random variables up to time \\(t\\). We consider \\({X_t}, {Y_t}\\) and \\({Z_t}\\) to be three stochastic processes. Let \\(\\hat Y_{t+1}\\) be a predictor for the value of the variable \\(Y\\) at time \\(t+1\\). We compare the expected value of a loss function \\(g(e)\\) with the error \\(e=\\hat{Y}_{t+1} - Y_{t+1}\\) of two models: The expected value of the prediction error given only \\(Y^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_1(X^{t},Z^t))] \\end{equation}\\] The expected value of the prediction error given \\(Y^t\\) and \\(X^t\\) \\[\\begin{equation} \\mathcal{R}(Y^{t+1} \\, | \\, X^{t},Y^t,Z^t) = \\mathbb{E}[g(Y_{t+1} - f_2(X^{t},Y^t,Z^t))]. \\end{equation}\\] In both models, the functions \\(f_1(.)\\) and \\(f_2(.)\\) are chosen to minimize the expected value of the loss function. In most cases, these functions are retrieved with linear and, possibly, with nonlinear regressions, neural networks etc. Typical forms for \\(g(.)\\) are the \\(l1\\)- or \\(l2\\)-norms. We can now provide our first definition of statistical causality under the Granger causal notion as follows: Definition 4.1 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(\\mathcal{R}(Y_{t+1} \\; | \\; X^t, Y^t, Z^t) = \\mathcal{R}(Y_{t+1} \\; | \\; Y^t, Z^t)\\). Standard Granger-causality tests assume a functional form in the relationship among the causes and effects and are implemented by fitting autoregressive models (Wiener 1956; Granger 1969). Consider the linear vector-autoregressive (VAR) equations: \\[\\begin{align} Y(t) &amp;= {\\alpha} + \\sum^k_{\\Delta t=1}{{\\beta}_{\\Delta t} Y(t-\\Delta t)} + \\epsilon_t, \\tag{4.1}\\\\ Y(t) &amp;= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} Y(t-\\Delta t)} + \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}X(t-\\Delta t)}+ \\widehat{\\epsilon}_t, \\tag{4.2} \\end{align}\\] where \\(k\\) is the number of lags considered. Alternatively, you can choose your DL/SVM/RF/GLM method of choice to fit the model. From Def. 4.1, \\(X\\) does not G-cause \\(Y\\) if and only if the prediction errors of \\(X\\) in the restricted Eq. (4.1) and unrestricted regression models Eq. (4.2) are equal (i.e., they are statistically indistinguishable). A one-way ANOVA test can be utilized to test if the residuals from Eqs. (4.1) and (4.2) differ from each other significantly. When more than one lag \\(k\\) is tested, a correction for multiple hypotheses testing should be applied, e.g. False Discovery Rate (FDR) or Bonferroni correction. 4.2 A Probabilistic-Based Definition A more general definition than Def. 4.1 that does not depend on assuming prediction functions can be formulated by considering conditional probabilities. A probabilistic definition of G-causality assumes that \\(Y_{t+1}\\) and \\(X^{t}\\) are independent given the past information \\((X^{t}, Y^{t})\\) if and only if \\(p(Y_{t+1} \\, | \\, X^{t}, Y^{t}, Z^{t}) = p(Y_{t+1} \\, | \\, Y^{t}, Z^{t})\\), where \\(p(. \\, | \\, .)\\) represents the conditional probability distribution. In other words, omitting past information from \\(X\\) does not change the probability distribution of \\(Y\\). This leads to our second definition of statistical causality as follows: Definition 4.2 \\(X\\) does not Granger-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(Y_{t+1} {\\perp\\!\\!\\!\\!\\perp}X^{t} \\; | \\; Y^{t}, Z^{t}\\). Def. 4.2 does not assume any functional form in the coupling between \\(X\\) and \\(Y\\). Nevertheless, it requires a method to assess their conditional dependency. In the next section, we will leverage an Information-Theoretical framework for that purpose. 4.3 Transfer Entropy and Statistical Causality Given a coupled system \\((X,Y)\\), where \\(P_Y(y)\\) is the pdf of the random variable \\(Y\\) and \\(P_{X,Y}\\) is the joint pdf between \\(X\\) and \\(Y\\), the joint entropy between \\(X\\) and \\(Y\\) is given by the following: \\[\\begin{equation} H(X,Y) = -\\sum_{x \\in X}{\\sum_{y \\in Y}{P_{X,Y}(x,y)\\log{P_{X,Y}(x,y)}}}. \\label{eq:HXY} \\end{equation}\\] The conditional entropy is defined by the following: \\[\\begin{equation} H\\left(Y\\middle\\vert X\\right) = H(X,Y) - H(X). \\end{equation}\\] We can interpret \\(H\\left(Y\\middle\\vert X\\right)\\) as the uncertainty of \\(Y\\) given a realization of \\(X\\). Figure 4.3: Shannon, Claude. The concept of information entropy was introduced by Claude Shannon in his 1948 paper: A Mathematical Theory of Communication. To compute G-Causality, we use the concept of Transfer Entropy. Since its introduction (Schreiber 2000), Transfer Entropy has been recognized as an important tool in the analysis of causal relationships in nonlinear systems (Hlavackovaschindler et al. 2007). It detects directional and dynamical information (Montalto 2014) while not assuming any particular functional form to describe interactions among systems. The Transfer Entropy can be defined as the difference between the conditional entropies: \\[\\begin{equation} TE\\left(X \\rightarrow Y\\right \\vert Z) = H\\left(Y^F\\middle\\vert Y^P,Z^P\\right) - H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right), \\tag{4.3} \\end{equation}\\] which can be rewritten as a sum of Shannon entropies: \\[\\begin{align} TE\\left(X \\rightarrow Y\\right) = H\\left(Y^P, X^P\\right) - H\\left(Y^F, Y^P, X^P\\right) + H\\left(Y^F, Y^P\\right) - H\\left(Y^P\\right), \\end{align}\\] where \\(Y^F\\) is a forward time-shifted version of \\(Y\\) at lag \\(\\Delta t\\) relatively to the past time-series \\(X^P\\), \\(Y^P\\) and \\(Z^P\\). Within this framework we say that \\(X\\) does not G-cause \\(Y\\) relative to side information \\(Z\\) if and only if \\(H\\left(Y^F\\middle\\vert Y^P,Z^P \\right) = H\\left(Y^F\\middle\\vert X^P, Y^P,Z^P\\right)\\), i.e., when \\(TE\\left(X \\rightarrow Y,Z^P\\right) = 0\\). 4.4 Net Information Flow Transfer-entropy is an asymmetric measure, i.e., \\(T_{X \\rightarrow Y} \\neq T_{Y \\rightarrow X}\\), and it thus allows the quantification of the directional coupling between systems. The Net Information Flow is defined as \\[\\begin{equation} \\widehat{TE}_{X \\rightarrow Y} = TE_{X \\rightarrow Y} - TE_{Y \\rightarrow X}\\;. \\end{equation}\\] One can interpret this quantity as a measure of the dominant direction of the information flow. In other words, a positive result indicates a dominant information flow from \\(X\\) to \\(Y\\) compared to the other direction or, similarly, it indicates which system provides more predictive information about the other system (Michalowicz, Nichols, and Bucholtz 2013). 4.5 The Link Between Granger-causality and Transfer Entropy It has been shown (Barnett, Barrett, and Seth 2009) that linear G-causality and Transfer Entropy are equivalent if all processes are jointly Gaussian. In particular, by assuming the standard measure (\\(l2\\)-norm loss function) of linear G-causality for the bivariate case as follows (see Section 4.1 for more details on linear-Granger causality): \\[\\begin{equation} GC_{X \\rightarrow Y} = \\log\\left( \\frac{var(\\epsilon_t)}{var( \\widehat{\\epsilon}_t)} \\right), \\tag{4.4} \\end{equation}\\] the following can be proved (Barnett, Barrett, and Seth 2009): \\[\\begin{align} TE_{X \\rightarrow Y} = GC_{X \\rightarrow Y}/2. \\tag{4.5} \\end{align}\\] This result provides a direct mapping between the Transfer Entropy and the linear G-causality implemented in the standard VAR framework. Hence, it is possible to estimate the TE both in its general form and with its equivalent form for linear G-causality. 4.6 Information Flow on Simulated Systems In this section, we construct simulated systems to couple random variables in a causal manner. We then quantify information flow using the methods studied in this Chapter. We first assume a linear system, where random variables have linear relationships defined as follow: \\[\\begin{align} x_1(n) &amp;= 0.95\\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\\\ \\nonumber x_2(n) &amp;= 0.5x_1(n-1) + w_2\\\\ \\nonumber x_3(n) &amp;= -0.4x_1(n-1) + w_3\\\\ \\nonumber x_4(n) &amp;= -0.5x_1(n-1) + 0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_4\\\\ \\nonumber x_5(n) &amp;= -0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_5, \\nonumber \\end{align}\\] where \\(w_1, w_2, w_3, w_4, w_5 \\sim N(0, 1)\\). To simulate this system we assume \\(x_i(0) = 0, i \\in (1, 2, \\ldots, 5)\\) as initial condition and then iteratively generate \\(x_i\\) for \\(n \\in (1, 2, \\ldots, N)\\) with a total of \\(N = 10,000\\) iterations by randomly sampling \\(w_i, i \\in (1, 2, \\ldots, 5)\\) from a normal distribution with zero mean and unit variance. We simulate this linear system with the following code: set.seed(123) n &lt;- 10000 x1 &lt;- x2&lt;-x3&lt;-x4&lt;-x5&lt;-rep(0, n + 1) for (i in 2:(n + 1)) { x1[i] &lt;- 0.95 * sqrt(2)* x1[i - 1] -0.9025*x1[i - 1] + rnorm(1, mean=0, sd=1) x2[i] &lt;- 0.5*x1[i - 1] + rnorm(1, mean=0, sd=1) x3[i] &lt;- -0.4*x1[i - 1] + rnorm(1, mean=0, sd=1) x4[i] &lt;- -0.5*x1[i - 1] + 0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) x5[i] &lt;- -0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) } x1 &lt;- x1[-1] x2 &lt;- x2[-1] x3 &lt;- x3[-1] x4 &lt;- x4[-1] x5 &lt;- x5[-1] linear.system &lt;- data.frame(x1, x2, x3, x4, x5) The Fig. 4.4 represents the dependencies of the simulated linear system. Figure 4.4: Interactions between the variables of the simulated linear system. We first define a function that calculates a bi-variate measure for G-causality as defined in Eq. (4.4) as follows: Linear.GC &lt;- function(X, Y){ n&lt;-length(X) X.now&lt;-X[1:(n-1)] Y.now&lt;-Y[1:(n-1)] Y.fut&lt;-Y[2:n] regression.uni=lm(Y.fut~Y.now) regression.mult=lm(Y.fut~Y.now+ X.now) var.eps.uni &lt;- (summary(regression.uni)$sigma)^2 var.eps.mult &lt;- (summary(regression.mult)$sigma)^2 GC &lt;- log(var.eps.uni/var.eps.mult) return(GC) } We use the function calc_te from the package RTransferEntropy (Behrendt et al. 2019) and the previously defined function Linear.GC to calculate pairwise information flow among the simulated variables as follows: library(RTransferEntropy) library(future) ## Allow for parallel computing plan(multiprocess) ## Warning: [ONE-TIME WARNING] Forked processing (&#39;multicore&#39;) is disabled ## in future (&gt;= 1.13.0) when running R from RStudio, because it is ## considered unstable. Because of this, plan(&quot;multicore&quot;) will fall ## back to plan(&quot;sequential&quot;), and plan(&quot;multiprocess&quot;) will fall back to ## plan(&quot;multisession&quot;) - not plan(&quot;multicore&quot;) as in the past. For more ## details, how to control forked processing or not, and how to silence this ## warning in future R sessions, see ?future::supportsMulticore ## Calculates GC and TE GC.matrix&lt;-FApply.Pairwise(linear.system, Linear.GC) TE.matrix&lt;-FApply.Pairwise(linear.system, calc_te) rownames(TE.matrix)&lt;-colnames(TE.matrix)&lt;-var.names&lt;-c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;) rownames(GC.matrix)&lt;-colnames(GC.matrix)&lt;-var.names ## Back to sequential plan(sequential) The function FApply.Pairwise is an auxiliary function that simply applies a given function D.Func to all possible pairs of columns from a given matrix X as follows: FApply.Pairwise &lt;- function(X, D.Func){ n = seq_len(ncol(X)) ff.TE.value = function(a, b) D.Func(X[,a], X[,b]) return(outer(n, n, Vectorize(ff.TE.value))) } Fig. 4.5 and Fig. 4.6 show Granger-causality and Transfer Entropy among the system’s variables, respectively. A cell \\((x, y)\\) presents the information flow from variable \\(y\\) to variable \\(x\\). We observe that both the Granger-causality (linear) and Transfer Entropy (nonlinear) approaches presented similar results, i.e., both methods captured the system’s dependencies similarly. This result is expected as the system is purely linear and Transfer Entropy is able to capture both the linear and nonlinear interactions. Figure 4.5: Granger-Causality of simulated linear system Figure 4.6: Transfer Entropy of simulated linear system We define a second system by introducing nonlinear interactions between \\(x_1\\) and the variables \\(x_2\\) and \\(x_5\\) as follows: \\[\\begin{align} x_1(n) &amp;= 0.95\\sqrt{2}x_1(n-1) - 0.9025x_1(n-1) + w_1\\\\ \\nonumber x_2(n) &amp;= 0.5x_1^2(n-1) + w_2\\\\ \\nonumber x_3(n) &amp;= -0.4x_1(n-1) + w_3\\\\ \\nonumber x_4(n) &amp;= -0.5x_1^2(n-1) + 0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_4\\\\ \\nonumber x_5(n) &amp;= -0.25\\sqrt{2}x_4(n-1) + 0.25\\sqrt{2}x_5(n-1) + w_5, \\nonumber \\end{align}\\] where \\(w_1, w_2, w_3, w_4\\) and \\(w_5 \\sim N(0, 1)\\). To simulate this system we assume \\(x_i(0) = 0, i \\in (1, 2, ..., 5)\\) as initial condition and then iteratively generate \\(x_i\\) for \\(n \\in (1, 2, ..., N)\\) with a total of \\(N = 10,000\\) iterations by randomly sampling \\(w_i, i \\in (1, 2, ..., 5)\\) from a normal distribution with zero mean and unit variance. We simulate this nonlinear system with the following code: set.seed(123) n &lt;- 10000 x1 &lt;- x2&lt;-x3&lt;-x4&lt;-x5&lt;-rep(0, n + 1) for (i in 2:(n + 1)) { x1[i] &lt;- 0.95 * sqrt(2)* x1[i - 1] -0.9025*x1[i - 1] + rnorm(1, mean=0, sd=1) x2[i] &lt;- 0.5*x1[i - 1]^2 + rnorm(1, mean=0, sd=1) x3[i] &lt;- -0.4*x1[i - 1] + rnorm(1, mean=0, sd=1) x4[i] &lt;- -0.5*x1[i - 1]^2 + 0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) x5[i] &lt;- -0.25*sqrt(2)*x4[i - 1] + 0.25*sqrt(2)*x5[i - 1] + rnorm(1, mean=0, sd=1) } x1 &lt;- x1[-1] x2 &lt;- x2[-1] x3 &lt;- x3[-1] x4 &lt;- x4[-1] x5 &lt;- x5[-1] nonlinear.system &lt;- data.frame(x1, x2, x3, x4, x5) Fig. 4.7 represents the dependencies of the simulated nonlinear system. Figure 4.7: Interactions between the variables of the simulated nonlinear system. We calculate Granger-causality and Transfer Entropy of the simulated nonlinear system as follows: ## Allow for parallel computing plan(multiprocess) ## Calculates GC and TE GC.matrix.nonlinear&lt;-FApply.Pairwise(nonlinear.system, Linear.GC) TE.matrix.nonlinear&lt;-FApply.Pairwise(nonlinear.system, calc_te) rownames(TE.matrix.nonlinear)&lt;-colnames(TE.matrix.nonlinear)&lt;-var.names rownames(GC.matrix.nonlinear)&lt;-colnames(GC.matrix.nonlinear)&lt;-var.names ## Back to sequential computing plan(sequential) From Fig. 4.8 and Fig. 4.9, we observe that the nonlinear interactions introduced were not captured by the linear form of the information flow. While all linear interactions presented similar linear and nonlinear information flows, the two nonlinear interactions introduced in the system presented relatively higher nonlinear information flow compared to the linear formulation. Figure 4.8: Granger-Causality of simulated nonlinear system Figure 4.9: Transfer Entropy of simulated nonlinear system 4.7 Information Flow among International Stock Market Indices The world’s financial markets form a complex, dynamic network in which individual markets interact with one another. This multitude of interactions can lead to highly significant and unexpected effects, and it is vital to understand precisely how various markets around the world influence one another (Junior, Mullokandov, and Kenett 2015). In this section, we use Transfer Entropy for the identification of dependency relations among international stock market indices. First, we select some of the major global indices for our analysis, namely the S&amp;P 500, the FTSE 100, the DAX, the EURONEXT 100 and the IBOVESPA, which track the following markets, respectively, the US, the UK, Germany, Europe and Brazil. They are defined by the following tickers: tickers&lt;-c(&quot;^GSPC&quot;, &quot;^FTSE&quot;, &quot;^GDAXI&quot;, &quot;^N100&quot;, &quot;^BVSP&quot;) Next, we will load log-returns of daily closing adjusted prices for the selected indices as follows (see Appendix B.1 for code used to generate this dataset): library(xts) dataset&lt;-as.xts(read.zoo(&#39;./data/global_indices_returns.csv&#39;, header=TRUE, index.column=1, sep=&quot;,&quot;)) head(dataset) ## X.GSPC X.FTSE X.GDAXI X.N100 X.BVSP ## 2000-01-02 19:00:00 0.000000 NA 0.00000 0.00000 0.00000 ## 2000-01-03 19:00:00 -0.039099 0.00000 -0.02456 -0.04179 -0.06585 ## 2000-01-04 19:00:00 0.001920 -0.01969 -0.01297 -0.02726 0.02455 ## 2000-01-05 19:00:00 0.000955 -0.01366 -0.00418 -0.00842 -0.00853 ## 2000-01-06 19:00:00 0.026730 0.00889 0.04618 0.02296 0.01246 ## 2000-01-09 19:00:00 0.011128 0.01570 0.02109 0.01716 0.04279 The influence that one market plays in another is dynamic. Here, we will consider the time period from 01/01/2014 until today and we will omit days with invalid returns due to bad data using the function NARV.omit from the package IDPmisc as follows: library(IDPmisc) dataset.post.crisis &lt;- NaRV.omit(as.data.frame(dataset[&quot;2014-01-01/&quot;])) We will calculate pairwise Transfer Entropy among all indices considered and construct a matrix such that each value in the position \\((i,j)\\) will contain the value Transfer Entropy from \\(tickers[i]\\) to \\(tickers[j]\\) as follows: ## Allow for parallel computing plan(multiprocess) # Calculate pairwise Transfer Entropy among global indices TE.matrix&lt;-FApply.Pairwise(dataset.post.crisis, calc_ete) rownames(TE.matrix)&lt;-colnames(TE.matrix)&lt;-tickers ## Back to sequential computing plan(sequential) Fig. 4.10 displays the resulting Transfer Entropy matrix. We normalize the Transfer Entropy values by dividing it by the maximum value in the matrix such that all values range from 0 to 1. We observe that the international indices studied are highly interconnected in the period analyzed with the highest information flow going from the US market to the UK market (^GSPC -&gt; ^FTSE). The second highest information flow is going from the UK market to the US market. That’s a result we would expect as the US and the UK markets are strongly coupled, historically. Figure 4.10: Normalized Transfer Entropy among international stock market indices. We also calculate the marginal contribution of each market to the total Transfer Entropy in the system by calculating the sum of Transfer Entropy for each row in the Transfer Entropy matrix, which we also normalize such that all values range from 0 to 1: TE.marginal&lt;-base::apply(TE.matrix, 1, sum) TE.marginal.norm&lt;-TE.marginal/sum(TE.marginal) print(TE.marginal.norm) ## ^GSPC ^FTSE ^GDAXI ^N100 ^BVSP ## 0.346 0.215 0.187 0.117 0.135 We observe that the US is the most influential market in the time period studied detaining 34.632% of the total Transfer Entropy followed by the UK and Germany with 21.472% and 18.653%, respectively. Japan and Brazil are the least influential markets with normalized Transfer Entropies of 11.744% and 13.498%, respectively. An experiment left to the reader is to build a daily trading strategy that exploits information flow among international markets. The proposed thesis is that one could build a profitable strategy by placing bets on futures of market indices that receive significant information flow from markets that observed unexpected returns/movements. For an extended analysis with a broader set of indices see (Junior, Mullokandov, and Kenett 2015). The authors develop networks of international stock market indices using an information theoretical framework. They use 83 stock market indices of a diversity of countries, as well as their single day lagged values, to probe the correlation and the flow of information from one stock index to another taking into account different operating hours. They find that Transfer Entropy is an effective way to quantify the flow of information between indices, and that a high degree of information flow between indices lagged by one day coincides to same day correlation between them. 4.8 Other Applications 4.8.1 Quantifying Information Flow Between Social Media and the Stock Market Investors’ decisions are modulated not only by companies’ fundamentals but also by personal beliefs, peers influence and information generated from news and the Internet. Rational and irrational investor’s behavior and their relation with the market efficiency hypothesis (Fama 1970) have been largely debated in the economics and financial literature (Shleifer 2000). However, it was only recently that the availability of vast amounts of data from online systems paved the way for the large-scale investigation of investor’s collective behavior in financial markets. A research paper (Souza and Aste 2016) used some of the methods studied in this Chapter to uncover that information flows from social media to stock markets revealing that tweets are causing market movements through a nonlinear complex interaction. The authors provide empirical evidence that suggests social media and stock markets have a nonlinear causal relationship. They take advantage of an extensive data set composed of social media messages related to DJIA index components. By using information-theoretic measures to cope for possible nonlinear causal effects between social media and the stock market, the work points out stunning differences in the results with respect to linear coupling. Two main conclusions are drawn: First, social media significant causality on stocks’ returns are purely nonlinear in most cases; Second, social media dominates the directional coupling with stock market, an effect not observable within linear modeling. Results also serve as empirical guidance on model adequacy in the investigation of sociotechnical and financial systems. Fig. 4.11 shows the significant causality links found between social media and stocks’ returns considering both cases: nonlinear (Transfer Entropy) and linear G-causality (linear VAR framework). The linear analysis discovers only three stocks with significant causality: INTEL CORP., NIKE INC. and WALT DISNEY CO. The Nonlinear analysis discovers that several other stocks have significant causality. In addition to the 3 stocks identified with significant linear causality, other 8 stocks presented purely nonlinear causality. Figure 4.11: Demonstration that the causality between social media and stocks’ returns are mostly nonlinear. Linear causality test indicated that social media caused stock’s returns only for 3 stocks. Nonparametric analysis showed that almost 1/3 of the stocks rejected in the linear case have significant nonlinear causality. In the nonlinear case, Transfer Entropy was used to quantify causal inference between the systems with randomized permutations test for significance estimation. In the linear case, a standard linear G-causality test was performed with a F-test under a linear vector-autoregressive framework. A significant linear G-causality was accepted if its linear specification was not rejected by the BDS test. p-values are adjusted with the Bonferroni correction. Significance is given at p-value &lt; 0.05. The low level of causality obtained under linear constraints is in-line with results from similar studies in the literature, where it was found that stocks’ returns show weak causality links (Alanyali, Moat, and Preis 2013, Antweiler and Frank (2004)) and social media sentiment analytics, at least when taken alone, have small or no predictive power (Ranco 2015) and do not have significant lead-time information about stock’s movements for the majority of the stocks (Zheludev, Smith, and Aste 2014). Contrariwise, results from the nonlinear analyses unveiled a much higher level of causality indicating that linear constraints may be neglecting the relationship between social media and stock markets. In summary, this paper (Souza and Aste 2016) is a good example on how causality can be not only complex but also misleading further highlighting the importance of choice in the methodology used to quantify it. 4.8.2 Detecting Causal Links Between Investor Sentiment and Cryptocurrency Prices In (Keskin and Aste 2019), the authors use information-theoretic measures studied in this Chapter for non-linear causality detection applied to social media sentiment and cryptocurrency prices. Using these techniques on sentiment and price data over a 48-month period to August 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple (XRP), litecoin (LTC) and ethereum (ETH), the authors detect significant information transfer, on hourly timescales, in directions of both sentiment to price and of price to sentiment. The work reports the scale of non-linear causality to be an order of magnitude greater than linear causality. The information-theoretic investigation detected a significant non-linear causal relationship in BTC, LTC and XRP, over multiple timescales and in both the directions sentiment to price and price to sentiment. The effect was strongest and most consistent for BTC and LTC. Fig. 4.12 shows Transfer Entropy results between BTC sentiment and BTC prices. Figure 4.12: Evidence that BTC sentiment and price are causally coupled in both directions in a non-linear way. Non-linear TE is calculated by multidimensional histograms with 6 quantile bins per dimension. Z-scores, calculated over 50 shuffles, show a high level of significance, especially during 2017 and 2018, in both directions. All analysis for this paper was performed using a Python package (PyCausality), which is available at https://github.com/ZacKeskin/PyCausality. 4.9 Conclusions Untangling cause and effect can be devilishly difficult. However, statistical tools can help us tell correlation from causation. In this Chapter, we introduced the notion of Granger-causality and its traditional implementation in a linear vector-autoregressive framework. We then defined information theoretical measures to quantify Transfer Entropy as a method to estimate statistical causality in nonlinear systems. We simulated linear and nonlinear systems further showing that the traditional linear G-causality approach failed to detect simple non-linearities introduced into the system while Transfer Entropy successfully detected such relationships. Finally, we showed how Transfer Entropy can be used to quantify relationships among global equity indexes. We also discussed further applications from the literature where Information Theoretical measures were used to quantify causal links between investor sentiment and movements in the equity and crypto markets. We hope you enjoyed this casual causal journey and remember: Quantify causality, responsibly. References "],
["financial-networks.html", "Chapter 5 Financial Networks 5.1 Introduction 5.2 Network Construction 5.3 Applications", " Chapter 5 Financial Networks 5.1 Introduction Financial markets can be regarded as a complex network in which nodes represent different financial assets and edges represent one or many types of relationships among those assets. Filtered correlation-based networks have successfully been used in the literature to study financial markets structure particularly from observational data derived from empirical financial time series (Bardoscia et al. 2017; S. A. L. Tumminello Michele AND Miccichè 2011; R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010, M. Tumminello et al. (2005)). The underlying principle is to use correlations from empirical financial time series to construct a sparse network representing the most relevant connections. Analyses on filtered correlation-based networks for information extraction (Song, Aste, and Di Matteo 2008; T. Aste, Shaw, and Di Matteo 2010) have widely been used to explain market interconnectedness from high-dimensional data. Applications include asset allocation (Y. Li et al. 2018; Pozzi, Di Matteo, and Aste 2013), market stability assessments (Morales et al. 2012), hierarchical structure analyses (R. N. Mantegna 1999; T. Aste, Shaw, and Di Matteo 2010; Michele Tumminello, Lillo, and Mantegna 2010; Musmeci, Aste, and Matteo 2014; Song, Di Matteo, and Aste 2012) and the identification of lead-lag relationships (Curme, Stanley, and Vodenska 2015). In this Chapter we will describe how to Construct and filter financial networks; Build price-based dynamic industry taxonomies; Implement a trading strategy based on financial network structure. 5.2 Network Construction We selected \\(N = 100\\) of the most capitalized companies that were part of the S&amp;P500 index from 09/05/2012 to 08/25/2017. The list of these companies’ ticker symbols is reported in the Appendix . For each stock \\(i\\) the financial variable was defined as the daily stock’s log-return \\(R_i(\\tau)\\) at time \\(\\tau\\). Stock returns \\(R_i\\) and social media opinion scores \\(O_i\\) each amounted to a time series of length equals to 1251 trading days. These series were divided time-wise into \\(M = 225\\) windows \\(t = 1, 2, \\ldots, M\\) of width \\(T = 126\\) trading days. A window step length parameter of \\(\\delta T = 5\\) trading days defined the displacement of the window, i.e., the number of trading days between two consecutive windows. The choice of window width \\(T\\) and window step \\(\\delta T\\) is arbitrary, and it is a trade-off between having analysis that is either too dynamic or too smooth. The smaller the window width and the larger the window steps, the more dynamic the data are. To characterize the synchronous time evolution of assets, we used equal time Kendall’s rank coefficients between assets \\(i\\) and \\(j\\), defined as \\[\\begin{equation} \\rho_{i, j}(t) = \\sum\\limits_{t&#39; &lt; \\tau}sgn(V_i(t&#39;) - V_i(\\tau))sgn(V_j(t&#39;) - V_j(\\tau)), \\end{equation}\\] where \\(t&#39;\\) and \\(\\tau\\) are time indexes within the window \\(t\\) and \\(V_i \\in \\{R_i, O_i\\}\\). Kendall’s rank coefficients takes into account possible nonlinear (monotonic) relationships. It fulfill the condition \\(-1 \\leq \\rho_{i, j} \\leq 1\\) and form the \\(N \\times N\\) correlation matrix \\(C(t)\\) that served as the basis for the networks constructed in this work. To construct the asset-based financial and social networks, we defined a distance between a pair of stocks. This distance was associated with the edge connecting the stocks, and it reflected the level at which they were correlated. We used a simple non-linear transformation \\(d_{i, j}(t) = \\sqrt{2(1 - \\rho_{i,j}(t))}\\) to obtain distances with the property \\(2 \\geq d_{i,j} \\geq 0\\), forming a \\(N \\times N\\) symmetric distance matrix \\(D(t)\\). 5.2.1 Network Filtering: Asset Graphs We extract the \\(N(N-1)/2\\) distinct distance elements from the upper triangular part of the distance matrix \\(D(t)\\), which were then sorted in an ascending order to form an ordered sequence \\(d_1(t), d_2(t), \\ldots, d_{N(N-1)/2}(t)\\). Since we require the graph to be representative of the market, it is natural to build the network by including only the strongest connections. This is a network filtering procedure that has been successfully applied in the construction of for the analyses of market structure . The number of edges to include is arbitrary, and we included those from the bottom quartile, which represented the 25% shortest edges in the graph (largest correlations), thus giving \\(E(t) = \\{d_1(t), d_2(t), \\ldots, d_{\\floor{N/4}}(t)\\}\\). We denoted \\(E^{F}(t)\\) as the set of edges constructed from the distance matrix derived from stock returns \\(R(t)\\). The financial network considered is \\(G^{F} = ( V, E^{F} )\\), where \\(V\\) is the vertex set of stocks. 5.2.2 Network Filtering: MST 5.2.3 Network Filtering: PMFG 5.3 Applications 5.3.1 Industry Taxonomy 5.3.2 Portfolio Construction References "],
["the-market-the-players-and-the-rules.html", "Chapter 6 The Market, The Players and The Rules 6.1 The Market 6.2 The Data 6.3 The Buyers 6.4 Conclusion", " Chapter 6 The Market, The Players and The Rules 6.1 The Market In an industry where almost every player has access to the same (or similar) core financial and fundamental data, investment firms look for Alternative Data seeking differentiated insights into companies not found in filings, earnings calls or fundamental datasets. Until recently, the usage of alternative data has been confined mainly to the realm of quantitative investment managers, as these firms were best suited to obtain, clean and process this data. Now, however, alternative data is beginning to go mainstream, with increasing interest from fundamental and hybrid asset managers here (Johnson 2019). Alternative Data is Untapped Alpha. The biggest opportunity for investors in this decade comes from the signals buried in the data generated by the digital economy. Alternative data is the deepest, least utilized alpha source in the world today - Quandl. On June/2019, alternative data went mainstream when Verizon made a bold move by launching a subscription service for Yahoo Finance that offers alternative data and insights to retail investors at a price point of $34.99/month. The amount spent in Alternative Data have been steadily increasing. More than half of all quantitative and fundamental investors surveyed recently are either considering or using alternative data as part of their workflow - Greenwich Associates. A recent Greenwich Associates report found that a wide majority of investment managers, 72%, stated that alternative data was improving enhanced their signal quality in an arena where filtering out signal noise. Of those who are implementing an alternative data strategy, more than one-fifth claim to have received 20% or more of their alpha from the practice. Figure 6.1: Total Buy-side spend on Alternative Data has been steadily increasing and its likely to nearly triple from 2018 to 2020. 6.2 The Data But where’s Alternative Data coming from and who are the key players providing access to it? Figure 6.2: Alternative Data are sourced by heterogeneous sources including individuals, businesses and sensors. The growth of Alternative Data has been enabled by the digitalization of the world around us. Data have been produced at an unprecedented rate by heterogeneous sources including: Individuals who today mirror their lives in the form digital behavior into the Web, Social Media and Apps; Sensors, particularly with the emergence of IoT, that generate satellite images, weather forecasts and geolocation data; Businesses from which data is generated in the form of traditional official filings and earning calls as well as banking records or supply chain data. Figure 6.3: The number of Alternative Data Providers is exploding. The number of Alternative Data Providers is exploding.The number of alternative data vendors is exploding particularly in the past decade. A few companies are worth mentioning: Quandl is likely the largest financial and alternative data aggregator/provider today. They leverage relationships with third-party providers to be a one-stop-shop for alternative data from consumers, IoT and sentiment to traditional fundamental, pricing and estimates. Quandl has been recently acquired by Nasdaq. Dataminr leverages advanced anomaly detection technology in a real-time AI platform to uncover critical events from social media before the information goes mainstream. The company raised a staggering $500MM+ amount in funding so far and it gives no sign of that is slowing down. Thinknum provides a SaaS-based web platform for a variety of alternative data sets, in particular, web-scrapped data such as product comparisons and company ratings on jobs sites. Thinkum claims to have 8 of the 10 largest investment banks as their clients. They recently closed a $11.6 million Series A funding to expand their financial modeling tools. Yewno is helping the world to uncover the undiscovered through its advanced dynamic Knowledge Graph and AI based inference engine, which introduces an entirely new approach to knowledge extraction to enhance human understanding by correlating concepts across a vast volume of sources. Yewno ingests data from sources such as clinical trials, patents, company transcripts and court opinions and more. Yewno raised about $30 million dollars in funding and it has launched AI-based equity indexes with major partners such as STOXX and Nasdaq and licensed alternative data feeds to buy-side firms. Other rising vendors include YipitData, 1010data and Enigma as well as incumbent players such as Refinitiv, S&amp;P Global Market Intelligence, Bloomberg and Factset. While the alternative data offering is exploding there is a gap between what data vendors offer and what data buyers want. A research report conducted by BattleFin and AlternativeData.org has found that there is a gap between what data vendors offer and what data buyers actually want: - Data buyers view the three most-valuable data categories as: credit/debit cards (14.83%) web data (12.29%), and social/sentiment (11.02%). - By contrast, the percentage of data providers covering these data categories are: credit card (0.69%), web-crawled data (3.94%) and sentiment (4.40%). - Data providers report that the top-three categories their data covers are: business insights (6.48%), web traffic (4.63%) and sentiment (4.40%). Figure 6.4: Data extracted from a research report conducted by BattleFin and AlternativeData.org, which compiles responses from 173 respondents made up of 69 alternative-data buyers and 104 data providers who responded to a 2018 survey conducted by BattleFin. 6.3 The Buyers The data buyers include your usual suspects such as Two Sigma, Milennium, Third Point, WorldQuant, ExodusPoint and emerging Quant Funds like Credit Suisse’s QT Fund. 80% of buyers expect to purchase a minimum of one to five datasets in 2019 while nearly 60 percent expects to test and evaluate a minimum of one to five datasets this year - according to a research report conducted by BattleFin and AlternativeData.org. Pension Plans and Index Providers are also entering the space seeking exposure to alternative factors, particularly with the emergence of AI-based strategies that leverage machine learning and computational linguistics techniques to extract signals from unstructured data (see STOXX’s or DWS’s AI-based indexes) as well as the increased interest in themes such as ESG (Environmental, Social and Corporate Governance) - a multi-dimensional multi-sourced theme with increasing demands that are no longer met by traditional data sources alone. I have been to a good number of meetings with data buyers (BattleFin is maybe the best event out there for 1-on-1 meetings with high-profile buy-side firms sometimes allowing for 30+ meetings in two days of events in addition to dozens of unofficial ones). Some questions are commonplace: Can you describe your data sources and your data ingestion process? What’s the frequency/history/coverage? What’s the delivery method? What are the key use cases for your data? Can I run a trial with live data? Who are your clients? What’s pricing like? Do you offer exclusivity? What is the size/funding/team profile of your firm? There is one tricky question though: Whether the data was already licensed to other clients. Particularly if the buyer is looking for alpha, they actually would prefer to be one of the first to test the data and avoid using a dataset that is already “crowded”. By the same token, they would be happy to hear that the dataset was already licensed to reputable clients as a sign of credibility. It’s up to you to navigate through this trade-off. Figure 6.5: The sales life-cycle of an alternative data product can take up to seven months from lead generation to customer sign-up. Image by Quandl. This is just the very first step in the sales and marketing life-cycle. From lead generation to customer sign-up, data buyers can take several months to make a final decision. Here are some findings: Providers and buyers agree that alt-data sales cycle can take up to seven months, with more than 75% of buyers requiring up to six months to test, evaluate and purchase an alt dataset. More than a third of data buyers use backtesting to inform their purchasing decision. Nearly 60% of providers are not interested in outsourcing the data testing process even if it can be handled efficiently. More than 20% of providers sell and distribute using a customized alt-data software platform. Nearly 18% use no software platform at all. 6.4 Conclusion Alternative Data, once a treat only for the most advanced Hedge Funds, is becoming a must-have in the alpha-generation war on Wall Street. Total spending in the area has been steadily growing as well as the number of data vendors and variety of data products. Relevant acquisitions have been made in the space and the first unicorn startups have been formed. However, there are critical challenges in the industry. There is a gap between what buyers find most relevant and what the data vendors are producing. Further, the sales life-cycle is long, as players are still figuring out how to test the value of the data and considerable time is spent on on-boarding new datasets. Successful players will be those who bridge the gap between data and insights first. The race has just begun, and it looks like no one wants to be just a spectator. References "],
["statistical-methods.html", "A Statistical Methods A.1 Kernel Density Estimation", " A Statistical Methods This Appendix provides details to some of statistical methods used in the book. A.1 Kernel Density Estimation In the entropy computation (see Section 3) the empirical probability distribution must be estimated. Histogram-based methods and kernel density estimations are the two main methods for that. Histogram-based is the simplest and most used nonparametric density estimator. Nonetheless, it yields density estimates that have discontinuities and vary significantly depending on the bin size choice. Also known as the Parzen-Rosenblatt window method, the kernel density estimation (KDE) approach approximates the density function at point \\(x\\) using neighboring observations. However, instead of building up the estimate according to the bin edges as in histograms, the KDE method uses each point of estimation \\(x\\) as the center of a bin of width \\(2h\\) and weight it according to a kernel function. Thereby, the kernel estimate of the probability density function \\(f(x)\\) is defined as \\[\\begin{equation} \\hat{f} = \\frac{1}{nh}\\sum_{x&#39; \\in X}{K\\left(\\frac{x - x&#39;}{h}\\right)}. \\tag{A.1} \\end{equation}\\] A usual choice for the kernel \\(K\\), which we use here, is the (Gaussian) radial basis function: \\[\\begin{equation} K(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}x^2}. \\end{equation}\\] The problem of selecting the bandwidth \\(h\\) in Eq. (A.1) is crucial in the density estimation. A large \\(h\\) will oversmooth the estimated density and mask the structure of the data. On the other hand, a small bandwidth will reduce the bias of the density estimate at the expense of a larger variance in the estimates. If we assume that the true distribution is Gaussian and we use a Gaussian kernel, the optimal value of \\(h\\) that minimizes the mean integrated squared error (MISE) is \\[\\begin{equation*} h^* = 1.06\\sigma N^{-1/5}, \\end{equation*}\\] where \\(N\\) is the total number of points and \\(\\sigma\\) can be estimated as the sample standard deviation. This bandwidth estimation is often called the Gaussian approximation or Silverman’s rule of thumb for kernel density estimation (Silverman and Green 1986). This is the most commonly-used method and it is here employed. Other common methods are given by (Sheather and Jones 1991) and (Scott 1992). References "],
["datasets.html", "B Datasets B.1 Log-Returns of International Stock Market Indices Prices", " B Datasets This Chapter provides code for datasets produced for the book. B.1 Log-Returns of International Stock Market Indices Prices B.1.1 Dataset Location ./data/global_indices_returns.csv B.1.2 Dataset Description Log-returns of adjusted prices for the indices identified by the following tickers: ^GSPC, ^FTSE, ^GDAXI, ^N100, ^BVSP B.1.3 Original Data Source Alpha Vantage B.1.4 Code library(quantmod) tickers&lt;-c(&quot;^GSPC&quot;, &quot;^FTSE&quot;, &quot;^GDAXI&quot;, &quot;^N100&quot;, &quot;^BVSP&quot;) data.env &lt;- new.env() dataset&lt;- xts() # Only run once # Download prices from AlphaVantage and calculate log-returns for(i in 1:length(tickers)) { tickers[i]-&gt; symbol print(symbol) getSymbols(symbol, src=&quot;av&quot;, auto.assign=TRUE, output.size=&quot;full&quot;, adjusted=TRUE, api.key=config::get()$alpha.vantage.key) dataset &lt;- merge(dataset, periodReturn(Ad(get(tickers[i])),period=&quot;daily&quot;, type=&#39;log&#39;)) rm(symbol) } names(dataset)&lt;-tickers tmp &lt;- tempfile() write.zoo(dataset,sep=&quot;,&quot;,file=&quot;./data/global_indices_returns.csv&quot;) B.1.5 Dataset Scheme "],
["r-evaltrue-echotrue-cachetrue-headdataset.html", "C {r eval=TRUE, echo=TRUE, cache=TRUE} # head(dataset) #", " C {r eval=TRUE, echo=TRUE, cache=TRUE} # head(dataset) # "],
["references.html", "References", " References "]
]
